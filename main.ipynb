{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ca974",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FEWSHOT_PROMPT_TEMPLATE = \"\"\"Predict the sentiment of the following input sentence.\n",
    "The response must begin with \"Sentiment: \", followed by one of these keywords: \"positive\", \"negative\", or \"neutral\", to reflect the sentiment of the input sentence.\n",
    "\n",
    "Here are the few examples:\n",
    "\n",
    "{few_shot_examples}\n",
    "\n",
    "Sentence: {input}\"\"\"\n",
    "\n",
    "def evaluate_few_shot(model, tokenizer, eval_dataset, few_shot_examples, batch_size=8, print_example=False):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    # format the few-shot examples for the prompt\n",
    "    formatted_few_shot_examples = \"\"\n",
    "    for i, example in enumerate(few_shot_examples):\n",
    "        formatted_few_shot_examples += f\"Sentence: {example['sentence']}\\nSentiment: {id2label[example['sentiment']]}\\n\"\n",
    "        if i < len(few_shot_examples) - 1:\n",
    "            formatted_few_shot_examples += \"\\n\"\n",
    "            \n",
    "    for i in range(0, len(eval_dataset), batch_size):\n",
    "        batch = eval_dataset[i:i + batch_size]\n",
    "        \n",
    "        # Replace USER_PROMPT_TEMPLATE with USER_FEWSHOT_PROMPT_TEMPLATE\n",
    "        user_prompts = [USER_FEWSHOT_PROMPT_TEMPLATE.format(input=sentence,\n",
    "                                                            few_shot_examples=formatted_few_shot_examples) for sentence in batch['sentence']]\n",
    "        \n",
    "        messages_list = [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant. You must fulfill the user request.\"},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ]\n",
    "            for user_prompt in user_prompts\n",
    "        ]\n",
    "        \n",
    "        input_prompts = [tokenizer.apply_chat_template(conversation=messages,\n",
    "                                                       add_generation_prompt=True,\n",
    "                                                       tokenize=False) for messages in messages_list]\n",
    "        inputs = tokenizer(input_prompts, return_tensors=\"pt\", padding=True, add_special_tokens=False)\n",
    "        \n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=16,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        output_ids = output_ids[:, inputs[\"input_ids\"][0].shape[-1]:output_ids.shape[-1]]\n",
    "        \n",
    "        predictions = tokenizer.batch_decode(outpud_ids, skip_special_tokens=True)\n",
    "        \n",
    "        if print_example:\n",
    "            print_example = False\n",
    "            print(f\"### Prompt: \\n{user_prompts[0]}\")\n",
    "            print(f\"### Prediction: \\n{predictions[0]}\")\n",
    "            print(f\"### Label: \\n{id2label[batch['sentiment'][0]]}\")\n",
    "        \n",
    "        pred_ids = []\n",
    "        true_labels = batch['sentiment']\n",
    "        for p, l in zip(predictions, true_labels):\n",
    "            try:\n",
    "                label_id = l\n",
    "                p = p.split(\":\")[-1].strip()\n",
    "                pred_id = label2id[p]\n",
    "            except Exception as e:\n",
    "                pred_id = (l + 1) % len(label2id)\n",
    "            pred_ids.append(pred_id)\n",
    "        \n",
    "        all_predictions.extend(pred_ids)\n",
    "        all_labels.extend(true_labels)\n",
    "        \n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_metric.compute(predictions=all_predictions, references=all_labels),\n",
    "        'f1': f1_metric.compute(predictions=all_predictions, references=all_labels, average='macro'),\n",
    "        'precision': precision_metric.compute(predictions=all_predictions, references=all_labels, average='macro'),\n",
    "        'recall': recall_metric.compute(predictions=all_predictions, references=all_labels, average='macro'),\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for metric_name, metric_dict in metrics.items():\n",
    "        results.update(metric_dict)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a list of shots from the train set\n",
    "shuffled_train_dataset = dataset['train'].shuffle()\n",
    "sampled_few_shot_examples = list(shuffled_train_dataset.select(range(10)))\n",
    "\n",
    "n_shots = [1, 2, 4, 8]\n",
    "for n in n_shots:\n",
    "    few_shot_examples = sampled_few_shot_examples[:n]\n",
    "    few_shot_results = evaluate_few_shot(\n",
    "        model=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        eval_dataset=dataset['test'],\n",
    "        few_shot_examples=few_shot_examples,\n",
    "        batch_size=16,\n",
    "        print_example=True,\n",
    "    )\n",
    "    print(f\"*** Few-shot evaluation results with {n} shots: \")\n",
    "    for metric_name, metric_value in few_shot_results.items():\n",
    "        print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016719f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lora(base_model, tokenizer, training_args, lora_rank, dataset):\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=lora_rank,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "    )\n",
    "    cur_peft_model = get_peft_model(base_model, peft_config)\n",
    "    cur_peft_model.print_trainable_parameters()\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=cur_peft_model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['validation'],\n",
    "        preprocess_logits_for_metric=preprocess_logits_for_metrics,\n",
    "        compute_metrics=compute_metrics,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    trainer.train()\n",
    "    return cur_peft_model\n",
    "\n",
    "ranks = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "rank_results = pd.DataFrame(columns=['rank', 'accuracy', 'f1', 'precision', 'recall'])\n",
    "\n",
    "for rank in ranks:\n",
    "    print(f\"*** Training with rank {rank} ***\")\n",
    "    cur_trained_model = train_lora(\n",
    "        base_model=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        training_args=training_args,\n",
    "        lora_rank=rank,\n",
    "        dataset=dataset,\n",
    "    )\n",
    "    \n",
    "    cur_results = evaluate_few_shot(\n",
    "        model=cur_trained_model,\n",
    "        tokenizer=tokenizer,\n",
    "        eval_dataset=dataset['test'],\n",
    "        few_shot_examples=sampled_few_shot_examples,\n",
    "        batch_size=16,\n",
    "    )\n",
    "    \n",
    "    # add current results to rank_results\n",
    "    rank_results.loc[len(rank_results)] = [rank, cur_results['accuracy'], cur_results['f1'], cur_results['precision'], cur_results['recall']]\n",
    "\n",
    "print(rank_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e5568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9def8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
