{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b781e539",
   "metadata": {},
   "source": [
    "# 4. Modeling \n",
    "\n",
    "+ Logistic Regression: likely to converge to a point due to its simplistic architecture, despite how many feature engineering you did \n",
    "\n",
    "+ MLP: The depth of the network allows for hierarchical feature learning, which is particularly useful for complex non-linearities. Each layer the model learns new features, but it is a black box and we as humans cannot interpret what those features are\n",
    "\n",
    "+ Tree-based algorithm: Gradient Boosting Machines (GBM) (e.g., XGBoost, LightGBM, CatBoost): These are also ensemble methods that build trees sequentially, with each new tree trying to correct the errors made by the previous ones. They are highly effective at capturing intricate non-linear patterns and often achieve state-of-the-art performance\n",
    "\n",
    "+ Support Vector Machines with non-linear kernels: By using kernel functions (like Radial Basis Function (RBF), polynomial, or sigmoid), SVMs can implicitly map the data into a higher-dimensional space where it might become linearly separable. This allows them to learn complex non-linear decision boundaries in the original feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad25f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import keras_tuner \n",
    "import gc\n",
    "import os # For directory creation\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Configuration ---\n",
    "N_SPLITS = 10       # Number of folds for final CV\n",
    "# N_EPOCHS = 100      # Max epochs for *final* training (can be adjusted after tuning)\n",
    "TUNER_MAX_EPOCHS = 500 # Max epochs for *each trial* during tuning (Hyperband manages this)\n",
    "TUNER_FACTOR = 3      # Reduction factor for Hyperband\n",
    "BATCH_SIZE = 356\n",
    "# L2_LAMBDA = 0.01    # Will be tuned\n",
    "# DROPOUT_RATE = 0.3  # Will be tuned\n",
    "EARLY_STOPPING_PATIENCE = 20 # Patience for final training AND tuner trials\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804e4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = pd.read_csv('train_processed_ver2.csv').set_index(\"participant_id\")\n",
    "test_metadata = pd.read_csv('test_processed_ver2.csv').set_index(\"participant_id\")\n",
    "\n",
    "train_fmri = pd.read_csv('train_fmri.csv').set_index(\"participant_id\")\n",
    "test_fmri = pd.read_csv('test_fmri.csv').set_index(\"participant_id\")\n",
    "\n",
    "train_merged = train_metadata.merge(train_fmri, left_index=True, right_index=True)\n",
    "test_merged = test_metadata.merge(test_fmri, left_index=True, right_index=True)\n",
    "\n",
    "y_train_df = pd.read_excel(\"data/TRAIN/TRAINING_SOLUTIONS.xlsx\").set_index(\"participant_id\")\n",
    "\n",
    "X_train_df = train_merged.sort_index()\n",
    "X_test_df = test_merged.sort_index()\n",
    "y_train_df = y_train_df.sort_index()\n",
    "\n",
    "# Get test participant IDs for later submission\n",
    "test_ids = pd.Series(X_test_df.index, name='participant_id')\n",
    "\n",
    "assert all(X_train_df.index == y_train_df.index), \"Label IDs do not match X_train_df IDs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Identify Numerical and OHE Columns (VERIFY THIS LIST) ---\n",
    "numerical_cols = ['EHQ_EHQ_Total', 'ColorVision_CV_Score', 'APQ_P_APQ_P_CP', 'APQ_P_APQ_P_ID', 'APQ_P_APQ_P_INV', 'APQ_P_APQ_P_OPD', 'APQ_P_APQ_P_PM', 'APQ_P_APQ_P_PP', 'SDQ_SDQ_Conduct_Problems', 'SDQ_SDQ_Difficulties_Total', 'SDQ_SDQ_Emotional_Problems', 'SDQ_SDQ_Externalizing', 'SDQ_SDQ_Generating_Impact', 'SDQ_SDQ_Hyperactivity', 'SDQ_SDQ_Internalizing', 'SDQ_SDQ_Peer_Problems', 'SDQ_SDQ_Prosocial', 'MRI_Track_Age_at_Scan', 'Barratt_Barratt_P1_Edu', 'Barratt_Barratt_P1_Occ', 'Barratt_Barratt_P2_Edu', 'Barratt_Barratt_P2_Occ']\n",
    "ohe_cols = [col for col in X_train_df.columns if col not in numerical_cols]\n",
    "\n",
    "assert len(numerical_cols) + len(ohe_cols) == len(X_train_df.columns)\n",
    "\n",
    "y = y_train_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c0ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for hyperparameter tuning\n",
    "X_hp_train_df, X_hp_val_df, y_hp_train, y_hp_val = train_test_split(\n",
    "    X_train_df, y,\n",
    "    test_size=0.2, # 20% for tuner validation\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y[:, 0] # Stratify by ADHD outcome\n",
    ")\n",
    "\n",
    "print(f\"HP Train shape: {X_hp_train_df.shape}, HP Val shape: {X_hp_val_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cfc83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scale Data for Hyperparameter Tuning ---\n",
    "scaler_hp = StandardScaler()\n",
    "X_hp_train_num_scaled = scaler_hp.fit_transform(X_hp_train_df[numerical_cols])\n",
    "X_hp_val_num_scaled = scaler_hp.transform(X_hp_val_df[numerical_cols])\n",
    "\n",
    "# Get OHE columns (no scaling)\n",
    "X_hp_train_ohe = X_hp_train_df[ohe_cols].values\n",
    "X_hp_val_ohe = X_hp_val_df[ohe_cols].values\n",
    "\n",
    "# Combine for tuner input\n",
    "X_hp_train_final = np.hstack((X_hp_train_num_scaled, X_hp_train_ohe))\n",
    "X_hp_val_final = np.hstack((X_hp_val_num_scaled, X_hp_val_ohe))\n",
    "\n",
    "# Prepare target dictionaries for tuner\n",
    "y_hp_train_dict = {'adhd_output': y_hp_train[:, 0], 'sex_output': y_hp_train[:, 1]}\n",
    "y_hp_val_dict = {'adhd_output': y_hp_val[:, 0], 'sex_output': y_hp_val[:, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c07f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_for_tuning(hp):\n",
    "    \"\"\"Builds a tunable multi-output neural network.\"\"\"\n",
    "    input_shape = (X_hp_train_final.shape[1],) # Use shape from prepared data\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # --- Tune Hyperparameters ---\n",
    "    hp_units_1 = hp.Int('units_1', min_value=32, max_value=128, step=16)\n",
    "    hp_units_2 = hp.Int('units_2', min_value=16, max_value=64, step=16)\n",
    "    hp_l2 = hp.Float('l2_lambda', min_value=1e-4, max_value=1e-1, sampling='log')\n",
    "    hp_dropout = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    x = layers.Dense(units=hp_units_1, activation='relu', kernel_regularizer=regularizers.l2(hp_l2))(inputs)\n",
    "    x = layers.Dropout(rate=hp_dropout)(x)\n",
    "    x = layers.Dense(units=hp_units_2, activation='relu', kernel_regularizer=regularizers.l2(hp_l2))(x)\n",
    "    x = layers.Dropout(rate=hp_dropout)(x)\n",
    "\n",
    "    output_adhd = layers.Dense(1, activation='sigmoid', name='adhd_output')(x)\n",
    "    output_sex = layers.Dense(1, activation='sigmoid', name='sex_output')(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=[output_adhd, output_sex])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss={'adhd_output': 'binary_crossentropy', 'sex_output': 'binary_crossentropy'},\n",
    "                  metrics={'adhd_output': ['accuracy'], # Keep metrics simple for tuner\n",
    "                           'sex_output': ['accuracy']})\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b787fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and Run the Tuner \n",
    "tuner = keras_tuner.Hyperband(\n",
    "    build_model_for_tuning,\n",
    "    objective='val_loss',\n",
    "    max_epochs=TUNER_MAX_EPOCHS,\n",
    "    factor=TUNER_FACTOR,\n",
    "    hyperband_iterations=1,\n",
    "    directory='keras_tuner_dir', \n",
    "    project_name='wids_datathon_tuning',\n",
    "    overwrite=True, \n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "tuner_early_stopping = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE)\n",
    "\n",
    "print(\"\\n--- Starting Hyperparameter Search ---\")\n",
    "tuner.search(X_hp_train_final, y_hp_train_dict,\n",
    "             epochs=TUNER_MAX_EPOCHS, # Max epochs *per trial*\n",
    "             batch_size=BATCH_SIZE,\n",
    "             validation_data=(X_hp_val_final, y_hp_val_dict),\n",
    "             callbacks=[tuner_early_stopping],\n",
    "             verbose=1 # Show progress for each trial\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"\\n--- Hyperparameter Search Complete ---\")\n",
    "print(f\"Best Hyperparameters Found:\")\n",
    "print(f\" - Units Layer 1: {best_hps.get('units_1')}\")\n",
    "print(f\" - Units Layer 2: {best_hps.get('units_2')}\")\n",
    "print(f\" - L2 Lambda: {best_hps.get('l2_lambda'):.5f}\")\n",
    "print(f\" - Dropout Rate: {best_hps.get('dropout_rate'):.2f}\")\n",
    "print(f\" - Learning Rate: {best_hps.get('learning_rate')}\")\n",
    "\n",
    "# Clean \n",
    "# del X_hp_train_df, X_hp_val_df, y_hp_train, y_hp_val\n",
    "# del X_hp_train_final, X_hp_val_final, y_hp_train_dict, y_hp_val_dict, scaler_hp\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5515d224",
   "metadata": {},
   "source": [
    "--- Hyperparameter Search Complete ---\n",
    "Best Hyperparameters Found:\n",
    " - Units Layer 1: 48\n",
    " - Units Layer 2: 64\n",
    " - L2 Lambda: 0.00013\n",
    " - Dropout Rate: 0.50\n",
    " - Learning Rate: 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406acfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Weighted F1 Score Function (Same as before) ---\n",
    "def calculate_weighted_f1(y_true, y_pred_proba, thresholds):\n",
    "    # (Function code remains identical to previous version)\n",
    "    threshold_adhd, threshold_sex = thresholds\n",
    "    y_pred_adhd = (y_pred_proba[:, 0] >= threshold_adhd).astype(int)\n",
    "    y_pred_sex = (y_pred_proba[:, 1] >= threshold_sex).astype(int)\n",
    "    y_true_adhd = y_true[:, 0]\n",
    "    y_true_sex = y_true[:, 1]\n",
    "    f1_sex = f1_score(y_true_sex, y_pred_sex, zero_division=0)\n",
    "    female_adhd_mask = (y_true_adhd == 1) & (y_true_sex == 1)\n",
    "    sample_weights = np.ones(len(y_true_adhd))\n",
    "    sample_weights[female_adhd_mask] = 2.0\n",
    "    tp_adhd = np.sum(sample_weights * (y_true_adhd == 1) * (y_pred_adhd == 1))\n",
    "    fp_adhd_unweighted = np.sum((y_true_adhd == 0) & (y_pred_adhd == 1))\n",
    "    fn_adhd = np.sum(sample_weights * (y_true_adhd == 1) * (y_pred_adhd == 0))\n",
    "    precision_adhd = tp_adhd / (tp_adhd + fp_adhd_unweighted) if (tp_adhd + fp_adhd_unweighted) > 0 else 0\n",
    "    recall_adhd = tp_adhd / (tp_adhd + fn_adhd) if (tp_adhd + fn_adhd) > 0 else 0\n",
    "    f1_adhd = (2 * precision_adhd * recall_adhd) / (precision_adhd + recall_adhd) if (precision_adhd + recall_adhd) > 0 else 0\n",
    "    final_score = (f1_adhd + f1_sex) / 2.0\n",
    "    return final_score\n",
    "\n",
    "# --- 4. Final Model Definition (Using Best HPs) ---\n",
    "# This function now builds the model with specific hyperparameters\n",
    "def build_final_model(input_shape, hps):\n",
    "    \"\"\"Builds the final model using the best hyperparameters.\"\"\"\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Get hyperparameters from the best_hps object\n",
    "    units_1 = hps.get('units_1')\n",
    "    units_2 = hps.get('units_2')\n",
    "    l2_lambda = hps.get('l2_lambda')\n",
    "    dropout_rate = hps.get('dropout_rate')\n",
    "    learning_rate = hps.get('learning_rate')\n",
    "\n",
    "    x = layers.Dense(units=units_1, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda))(inputs)\n",
    "    x = layers.Dropout(rate=dropout_rate)(x)\n",
    "    x = layers.Dense(units=units_2, activation='relu', kernel_regularizer=regularizers.l2(l2_lambda))(x)\n",
    "    x = layers.Dropout(rate=dropout_rate)(x)\n",
    "\n",
    "    output_adhd = layers.Dense(1, activation='sigmoid', name='adhd_output')(x)\n",
    "    output_sex = layers.Dense(1, activation='sigmoid', name='sex_output')(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=[output_adhd, output_sex])\n",
    "\n",
    "    # Compile using the best learning rate\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss={'adhd_output': 'binary_crossentropy', 'sex_output': 'binary_crossentropy'},\n",
    "                  # Include more metrics for monitoring final training if desired\n",
    "                  metrics={'adhd_output': ['accuracy', tf.keras.metrics.Precision(name='prec_adhd'), tf.keras.metrics.Recall(name='rec_adhd')],\n",
    "                           'sex_output': ['accuracy', tf.keras.metrics.Precision(name='prec_sex'), tf.keras.metrics.Recall(name='rec_sex')]})\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Cross-Validation and Training (Using Best HPs) ---\n",
    "print(\"\\n--- Starting Cross-Validation with Best Hyperparameters ---\")\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "oof_preds_df = pd.DataFrame(np.zeros((len(X_train_df), 2)), index=X_train_df.index, columns=['ADHD_Outcome', 'Sex_F'])\n",
    "fold_scores = []\n",
    "X_test_scaled_list = [] # Store test predictions from each fold\n",
    "\n",
    "# Use the full training data (X_train_df, y) for K-Fold\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_df, y[:, 0])):\n",
    "    print(f\"\\n--- Fold {fold + 1}/{N_SPLITS} ---\")\n",
    "\n",
    "    X_train_fold_df = X_train_df.iloc[train_idx]\n",
    "    X_val_fold_df = X_train_df.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Feature Scaling (within the fold)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_fold_num_scaled = scaler.fit_transform(X_train_fold_df[numerical_cols])\n",
    "    X_val_fold_num_scaled = scaler.transform(X_val_fold_df[numerical_cols])\n",
    "    X_train_fold_ohe = X_train_fold_df[ohe_cols].values\n",
    "    X_val_fold_ohe = X_val_fold_df[ohe_cols].values\n",
    "    X_train_fold_final = np.hstack((X_train_fold_num_scaled, X_train_fold_ohe))\n",
    "    X_val_fold_final = np.hstack((X_val_fold_num_scaled, X_val_fold_ohe))\n",
    "\n",
    "    # --- Build final model using the BEST hyperparameters found ---\n",
    "    model = build_final_model(input_shape=(X_train_fold_final.shape[1],), hps=best_hps)\n",
    "\n",
    "    # Use a suitable number of epochs for final training - could be fixed, or based on tuner results\n",
    "    # Let's use a higher number than the tuner's max_epochs, but with early stopping\n",
    "    FINAL_TRAINING_EPOCHS = 80\n",
    "    final_early_stopping = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True, verbose=1)\n",
    "\n",
    "    y_train_fold_dict = {'adhd_output': y_train_fold[:, 0], 'sex_output': y_train_fold[:, 1]}\n",
    "    y_val_fold_dict = {'adhd_output': y_val_fold[:, 0], 'sex_output': y_val_fold[:, 1]}\n",
    "\n",
    "    history = model.fit(X_train_fold_final, y_train_fold_dict,\n",
    "                        epochs=FINAL_TRAINING_EPOCHS, # Train potentially longer now\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        validation_data=(X_val_fold_final, y_val_fold_dict),\n",
    "                        callbacks=[final_early_stopping], # Use early stopping for final training\n",
    "                        verbose=1)\n",
    "\n",
    "    # Predict probabilities on validation set for OOF\n",
    "    val_preds_proba = model.predict(X_val_fold_final)\n",
    "    oof_preds_df.iloc[val_idx, 0] = val_preds_proba[0].flatten()\n",
    "    oof_preds_df.iloc[val_idx, 1] = val_preds_proba[1].flatten()\n",
    "\n",
    "    # Scale and predict on the test set\n",
    "    X_test_num_scaled_fold = scaler.transform(X_test_df[numerical_cols])\n",
    "    X_test_ohe_fold = X_test_df[ohe_cols].values\n",
    "    X_test_final_fold = np.hstack((X_test_num_scaled_fold, X_test_ohe_fold))\n",
    "    test_preds_fold = model.predict(X_test_final_fold)\n",
    "    X_test_scaled_list.append(np.hstack(test_preds_fold))\n",
    "\n",
    "    fold_score_temp = calculate_weighted_f1(y_val_fold, np.hstack(val_preds_proba), thresholds=(0.5, 0.5))\n",
    "    print(f\"Fold {fold + 1} preliminary validation score (0.5 threshold): {fold_score_temp:.4f}\")\n",
    "    fold_scores.append(fold_score_temp)\n",
    "\n",
    "    del model, scaler, X_train_fold_df, X_val_fold_df, X_train_fold_final, X_val_fold_final\n",
    "    gc.collect()\n",
    "    keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6091d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nCV finished. Average preliminary score (0.5 threshold): {np.mean(fold_scores):.4f}\")\n",
    "oof_preds = oof_preds_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f2871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Threshold Optimization (Using OOF from Best HP models) ---\n",
    "print(\"\\n--- Optimizing Thresholds using OOF Predictions ---\")\n",
    "# (Code remains identical to previous version)\n",
    "best_score = -1\n",
    "best_thresholds = (0.5, 0.5)\n",
    "threshold_space = np.linspace(0.1, 0.9, 41)\n",
    "for t_adhd in threshold_space:\n",
    "    for t_sex in threshold_space:\n",
    "        current_score = calculate_weighted_f1(y, oof_preds, thresholds=(t_adhd, t_sex))\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_thresholds = (t_adhd, t_sex)\n",
    "            \n",
    "print(f\"Best OOF Weighted F1 Score: {best_score:.5f}\")\n",
    "print(f\"Optimal Thresholds found: ADHD={best_thresholds[0]:.3f}, Sex={best_thresholds[1]:.3f}\")\n",
    "\n",
    "# --- 7. Final Test Prediction (Averaging Best HP model predictions) ---\n",
    "print(\"\\n--- Averaging Test Predictions from CV Folds ---\")\n",
    "final_test_preds_proba = np.mean(X_test_scaled_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Generate Submission File (Same as before) ---\n",
    "print(\"\\n--- Generating Submission File ---\")\n",
    "# (Code remains identical to previous version)\n",
    "pred_adhd_final = (final_test_preds_proba[:, 0] >= best_thresholds[0]).astype(int)\n",
    "pred_sex_final = (final_test_preds_proba[:, 1] >= best_thresholds[1]).astype(int)\n",
    "submission_df = pd.DataFrame({'participant_id': test_ids, 'ADHD_Outcome': pred_adhd_final, 'Sex_F': pred_sex_final})\n",
    "submission_path = 'submission_tuned.csv' # Changed filename\n",
    "submission_df.to_csv(submission_path, index=False, float_format='%.1f')\n",
    "print(f\"Submission file saved to: {submission_path}\")\n",
    "print(\"Top 5 rows of submission file:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4591c131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
