{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f57a0cb7",
   "metadata": {},
   "source": [
    "# WiDS Datathon 2025 - 1st place solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93604ba1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87cd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -q fastparquet\n",
    "%pip install -q feature-engine\n",
    "%pip install -q shap\n",
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b493dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initial setup\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor, MultiOutputClassifier # Model nhị phân song song \n",
    "from sklearn.metrics import f1_score as f1_score_calc, roc_curve, roc_auc_score, accuracy_score, confusion_matrix, multilabel_confusion_matrix\n",
    "\n",
    "# Ensemble models\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier, Pool\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "pd.options.display.max_columns = 60\n",
    "\n",
    "import shap\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba968bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q = pd.read_excel(\"data/train/TRAIN_QUANTITATIVE_METADATA_new.xlsx\")\n",
    "train_c = pd.read_excel(\"data/train/TRAIN_CATEGORICAL_METADATA_new.xlsx\")\n",
    "test_q = pd.read_excel(\"data/test/TEST_CATEGORICAL_METADATA.xlsx\")\n",
    "test_c = pd.read_excel(\"data/test/TEST_QUANTITATIVE_METADATA.xlsx\")\n",
    "sample = pd.read_excel(\"data/SAMPLE_SUBMISSION.xlsx\")\n",
    "\n",
    "# Use parquet for faster loading\n",
    "train_fcm = pd.read_parquet(\"data/train/train_fmri.parquet\", engine='fastparquet')\n",
    "test_fcm = pd.read_parquet(\"data/test/test_fmri.parquet\", engine='fastparquet')\n",
    "\n",
    "train_raw = pd.merge(train_q, train_c, on=\"participant_id\", how=\"left\")\n",
    "test_raw = pd.merge(test_q, test_c, on=\"participant_id\", how=\"left\")\n",
    "\n",
    "labels = pd.read_excel(\"data/train/TRAINING_SOLUTIONS.xlsx\")\n",
    "\n",
    "train_raw['weight'] = 1\n",
    "test_raw['weight'] = 1\n",
    "train_raw = pd.merge(train_raw, labels, on=\"participant_id\", how=\"left\")\n",
    "\n",
    "train_raw.loc[(train_raw['ADHD_Outcome']==1) & (train_raw['Sex_F']==1), \"weight\"] = 2\n",
    "\n",
    "# backup\n",
    "train_df = train_raw.copy()\n",
    "test_df = test_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f7ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681efc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast check target\n",
    "sns.countplot(data=train_df, x='ADHD_Outcome', hue='Sex_F')\n",
    "plt.title('ADHD Outcome Count by Sex')\n",
    "plt.xlabel('ADHD Outcome (0=Other/None, 1=ADHD)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Sex (0=Male, 1=Female)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587b90e",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing\n",
    "\n",
    "+ Remove useless features: `participant_id`, `Basic_Demos_Enroll_Year`, `Basic_Demos_Study_Site`, `MRI_Track_Scan_Location`\n",
    "+ Create new features: `edu_diff`, `occ_diff`\n",
    "+ Apply `Min-Max Scaling` for SDQ and APQ features\n",
    "+ Create cumulative features and percentage features for SDQ and APQ\n",
    "+ Label encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7920b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum() / train_df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de3281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill nan\n",
    "imputer = IterativeImputer(estimator=LassoCV(random_state=42, n_jobs=8, cv=8), max_iter=5, random_state=42, verbose=2)\n",
    "cols_to_impute = train_df.columns.drop(['participant_id', \"ADHD_Outcome\", 'Sex_F'])\n",
    "\n",
    "train_df = train_df[cols_to_impute]\n",
    "test_df = test_df[cols_to_impute]\n",
    "\n",
    "imputed_data = imputer.fit_transform(train_df)\n",
    "train_df[cols_to_impute] = imputed_data[:len(train_df), :]\n",
    "test_df[cols_to_impute] = imputer.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a866dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connectome Matrices\n",
    "train_fcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_participant_ids = train_raw['participant_id']\n",
    "test_participant_ids = test_raw['participant_id']\n",
    "ahdh_labels = train_raw['ADHD_Outcome']\n",
    "sex_labels = train_raw['Sex_F']\n",
    "\n",
    "train_df['participant_id'] = train_participant_ids\n",
    "train_df['ADHD_Outcome'] = ahdh_labels\n",
    "train_df['Sex_F'] = sex_labels \n",
    "\n",
    "test_df['participant_id'] = test_participant_ids\n",
    "\n",
    "assert 'participant_id' in train_raw.columns\n",
    "assert 'ADHD_Outcome' in train_raw.columns\n",
    "assert 'Sex_F' in train_raw.columns\n",
    "assert 'participant_id' in test_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7bf8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with functional connectome matrices\n",
    "df_merge = pd.merge(train_df, train_fcm, on=\"participant_id\", how=\"left\")\n",
    "test_merge = pd.merge(test_df, test_fcm, on=\"participant_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b03bca",
   "metadata": {},
   "source": [
    "# 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d8c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1(y_true, y_pred, threshold=0.5, weight=None):\n",
    "    x = f1_score_calc(y_true, (y_pred > threshold).astype(int), sample_weight=weight)\n",
    "    # print(x)\n",
    "    return x\n",
    "\n",
    "def balanced_kfold_split(df_xx, df_x, n_splits=1, group_column='ADHD_Outcome', df_buff=None, seed=SEED):\n",
    "    \"\"\"\n",
    "    Custom balanced k-fold split for a dataframe ensuring balanced distribution for each groups\n",
    "    Args:\n",
    "        df_xx: DataFrame to be split (containing input features only)\n",
    "        df_x: Origin DataFrame containing the target variable\n",
    "        n_splits: Number of splits\n",
    "        group_column: Column name to be used for grouping\n",
    "        df_buff: Buffer DataFrame to be used in all folds\n",
    "    \"\"\"\n",
    "    if n_splits==1:\n",
    "        return [[df_xx.index, df_xx.index]]\n",
    "    \n",
    "    # Align group column from df_x to df\n",
    "    df = df_xx.copy()\n",
    "    df[group_column] = df_x[group_column].values  # Ensure correct order of target in df\n",
    "    buffer_indices = None\n",
    "    if df_buff is not None:\n",
    "        buffer_indices = df_buff.index.tolist()\n",
    "\n",
    "    # Initialize folds\n",
    "    folds = [[] for _ in range(n_splits)]\n",
    "    groups = df[group_column].unique()\n",
    "\n",
    "    for group in groups:\n",
    "        group_indices = df[df[group_column] == group].index.tolist()\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(group_indices)  # Shuffle indices for the group\n",
    "\n",
    "        # Distribute the group indices across folds\n",
    "        for i, idx in enumerate(group_indices):\n",
    "            folds[i % n_splits].append(idx)\n",
    "\n",
    "    # Create train and validation indices for each fold\n",
    "    splits = []\n",
    "    for i in range(n_splits):\n",
    "        val_indices = np.array(folds[i])\n",
    "        train_indices = np.array([idx for fold in folds if fold != folds[i] for idx in fold])\n",
    "        if buffer_indices:\n",
    "            buffer_indices_tmp = [idx for idx in buffer_indices if idx not in train_indices]\n",
    "            print('buff', len(buffer_indices_tmp))\n",
    "            train_indices = np.array(list(train_indices)+buffer_indices_tmp)\n",
    "        splits.append((train_indices, val_indices))\n",
    "    np.random.seed(42)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c909ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(df_x, target_cols=[]):\n",
    "    print('Preparing data...')\n",
    "    df = df_x.copy()\n",
    "\n",
    "    ignore_columns = [\n",
    "                    'participant_id',\n",
    "                    'Basic_Demos_Enroll_Year',\n",
    "                    'Basic_Demos_Study_Site',\n",
    "                    'MRI_Track_Scan_Location',\n",
    "                ]\n",
    "    \n",
    "    ignore_columns = [x for x in ignore_columns if x in df.columns]\n",
    "    if ignore_columns:\n",
    "        df = df.drop(ignore_columns, axis=1, errors=\"ignore\")\n",
    "\n",
    "    # Some feature engineering, main purpose is boost for ADHD_Outcome model,\n",
    "    # Sex_F model mainly depends on functional connectome matrices\n",
    "    df[\"edu_diff\"] = df[\"Barratt_Barratt_P1_Edu\"] - df[\"Barratt_Barratt_P2_Edu\"]\n",
    "    df[\"occ_diff\"] = df[\"Barratt_Barratt_P1_Occ\"] - df[\"Barratt_Barratt_P2_Occ\"]\n",
    "\n",
    "    sdq_cols = [c for c in df.columns if c.startswith(\"SDQ_SDQ_\")]\n",
    "    apq_cols = [c for c in df.columns if c.startswith(\"APQ_P_APQ_P\")]\n",
    "\n",
    "    # Normalize SDQ and APQ columns to [0, 1]\n",
    "    # Tree base model not affect by scale, target only to calculate percent for each column across all columns in the group.\n",
    "    scaler = MinMaxScaler()\n",
    "    df.loc[:, sdq_cols] = scaler.fit_transform(df[sdq_cols])\n",
    "    df.loc[:, apq_cols] = scaler.fit_transform(df[apq_cols])\n",
    "\n",
    "    df['SDQ_SDQ_sum'] = df.filter(like='SDQ_SDQ').sum(axis=1)\n",
    "    df['APQ_P_APQ_sum'] = df.filter(like='APQ_P_APQ').sum(axis=1)\n",
    "\n",
    "    for col in apq_cols:\n",
    "        df[f\"{col}_percent\"] = df[col]/df['APQ_P_APQ_sum']\n",
    "    for col in sdq_cols:\n",
    "        df[f\"{col}_percent\"] = df[col]/df['SDQ_SDQ_sum']\n",
    "\n",
    "    # Separate features and labels\n",
    "    TARGET_COLUMNS = target_cols\n",
    "    if set(TARGET_COLUMNS).issubset(df.columns.tolist()):\n",
    "        feature_train = df\n",
    "        target_train = df[TARGET_COLUMNS].copy()**(1/1)\n",
    "    else:\n",
    "        feature_train = df\n",
    "\n",
    "    # Process categorical columns\n",
    "    categorical_columns=[\n",
    "            \"Basic_Demos_Study_Site\",\n",
    "            \"MRI_Track_Scan_Location\",\n",
    "            \"Basic_Demos_Enroll_Year\",\n",
    "            \"PreInt_Demos_Fam_Child_Ethnicity\",\n",
    "            \"PreInt_Demos_Fam_Child_Race\",\n",
    "            \"Barratt_Barratt_P1_Occ\",\n",
    "            \"Barratt_Barratt_P2_Occ\",\n",
    "            \"Barratt_Barratt_P1_Edu\",\n",
    "            \"Barratt_Barratt_P2_Edu\",\n",
    "            ]\n",
    "\n",
    "    print('categorical_columns:', categorical_columns)\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        if col in feature_train.columns:\n",
    "            feature_train[col] = feature_train[col].round(0).astype(int) # better when round filled nan values with nearest int\n",
    "            feature_train[col] = feature_train[col].astype('category')\n",
    "            feature_train[col] = feature_train[col].cat.codes\n",
    "            feature_train[col] = feature_train[col].astype('category')\n",
    "        else:\n",
    "            print('NO column', col)\n",
    "\n",
    "    feature_train = feature_train.reset_index(drop=True)\n",
    "\n",
    "    print('Done data process')\n",
    "    return feature_train, target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b61dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    th = {\"ADHD_Outcome\": 0.8, \"Sex_F\": 0.14}\n",
    "    target_cols = ['ADHD_Outcome']\n",
    "\n",
    "CONFIG.target_cols = [\"Sex_F\"]\n",
    "CONFIG.target_cols = [\"ADHD_Outcome\"]\n",
    "CONFIG.target_cols = [\"ADHD_Outcome\", \"Sex_F\"]\n",
    "\n",
    "def get_data(df, test, save_out=False):\n",
    "    # 2.1 Kết hợp train + test\n",
    "    combined = pd.concat([df,test],axis=0,ignore_index=True)\n",
    "    print(len(combined))\n",
    "\n",
    "    feature_train, target_train = data_process(combined, target_cols=CONFIG.target_cols)\n",
    "    print(len(feature_train))\n",
    "\n",
    "    feature_test = feature_train.iloc[len(df):].reset_index(drop=True).copy()\n",
    "    print(len(feature_test))\n",
    "\n",
    "    feature_train = feature_train.iloc[:len(df)].copy()\n",
    "    feature_train.drop([x for x in [\"ADHD_Outcome\", \"Sex_F\"] if x not in CONFIG.target_cols], axis=1, inplace=True)\n",
    "\n",
    "    feature_test.drop([\"ADHD_Outcome\", \"Sex_F\"], axis=1, inplace=True)\n",
    "    feature_test.drop([\"weight\"], axis=1, inplace=True)\n",
    "\n",
    "    weights_train = feature_train[['weight']]\n",
    "\n",
    "    feature_train.drop([\"weight\"], axis=1, inplace=True)\n",
    "    target_train = target_train.iloc[:len(df)].copy()\n",
    "\n",
    "    if save_out:\n",
    "        feature_train.to_parquet(\"data/train/feature_train.parquet\",\n",
    "                                 compression=None,\n",
    "                                 engine=\"fastparquet\")\n",
    "        feature_test.to_parquet(\"data/test/feature_test.parquet\",\n",
    "                                compression=None,\n",
    "                                engine=\"fastparquet\")\n",
    "        weights_train.to_parquet(\"data/train/weights_train.parquet\",\n",
    "                                compression=None,\n",
    "                                engine=\"fastparquet\")\n",
    "        df.to_parquet(\"data/df.parquet\", compression=None)\n",
    "    return feature_train, target_train, weights_train, feature_test\n",
    "\n",
    "feature_train, target_train, weights_train, feature_test = get_data(df_merge, test_merge, save_out=True)\n",
    "combined = pd.concat([feature_train,feature_test],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce52ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933c62d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262dd3a6",
   "metadata": {},
   "source": [
    "# 4. Modeling\n",
    "\n",
    "This is the main training loop:\n",
    "1. Create k-fold splits\n",
    "2. Initialize the configuration with target columns and thresholds\n",
    "3. Loop through target variables\n",
    "4. For each target and model type, train models on all folds\n",
    "5. Ensemble predictions from different models and folds\n",
    "6. Generate final predictions using optimized thresholds\n",
    "7. Outputs feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "from sklearn.base import (\n",
    "    _fit_context,\n",
    "    clone,\n",
    "    is_classifier,\n",
    ")\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn.utils.metadata_routing import (\n",
    "    _routing_enabled,\n",
    "    process_routing,\n",
    ")\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "from sklearn.utils.parallel import Parallel as sk_Parallel, delayed as sk_delayed\n",
    "from sklearn.utils.validation import (\n",
    "    _check_method_params,\n",
    "    check_is_fitted,\n",
    "    has_fit_parameter,\n",
    ")\n",
    "\n",
    "def _fit_estimator(estimator, X, y, sample_weight=None, set_params=None, **fit_params):\n",
    "    \"\"\"Fit the estimator and return it.\"\"\"\n",
    "    estimator = clone(estimator)\n",
    "    if set_params:\n",
    "        if hasattr(estimator, \"set_params\"):\n",
    "            estimator.set_params(**set_params)\n",
    "        else:\n",
    "            print(\"No set_params\")\n",
    "            \n",
    "    if sample_weight is not None:\n",
    "        estimator.fit(X, y, sample_weight=sample_weight, **fit_params)\n",
    "    else:\n",
    "        estimator.fit(X, y, **fit_params)\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e240636",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMultiOutputRegressor(MultiOutputRegressor):\n",
    "\n",
    "    @_fit_context(\n",
    "        # MultiOutput*.estimator is not validated yet\n",
    "        prefer_skip_nested_validation=False\n",
    "    )\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None, custom_allow_cols=None, custom_set_params=[], **fit_params):\n",
    "        \"\"\"Fit the model to data, separately for each output variable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        y : {array-like, sparse matrix} of shape (n_samples, n_outputs)\n",
    "            Multi-output targets. An indicator matrix turns on multilabel\n",
    "            estimation.\n",
    "\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Sample weights. If `None`, then samples are equally weighted.\n",
    "            Only supported if the underlying regressor supports sample\n",
    "            weights.\n",
    "\n",
    "        **fit_params : dict of string -> object\n",
    "            Parameters passed to the ``estimator.fit`` method of each step.\n",
    "\n",
    "            .. versionadded:: 0.23\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns a fitted instance.\n",
    "        \"\"\"\n",
    "        if not hasattr(self.estimator, \"fit\"):\n",
    "            raise ValueError(\"The base estimator should implement a fit method\")\n",
    "\n",
    "        y = self._validate_data(X=\"no_validation\", y=y, multi_output=True)\n",
    "\n",
    "        if is_classifier(self):\n",
    "            check_classification_targets(y)\n",
    "\n",
    "        if y.ndim == 1:\n",
    "            raise ValueError(\n",
    "                \"y must have at least two dimensions for \"\n",
    "                \"multi-output regression but has only one.\"\n",
    "            )\n",
    "\n",
    "        if _routing_enabled():\n",
    "            if sample_weight is not None:\n",
    "                fit_params[\"sample_weight\"] = sample_weight\n",
    "            routed_params = process_routing(\n",
    "                self,\n",
    "                \"fit\",\n",
    "                **fit_params,\n",
    "            )\n",
    "        else:\n",
    "            if sample_weight is not None and not has_fit_parameter(\n",
    "                self.estimator, \"sample_weight\"\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    \"Underlying estimator does not support sample weights.\"\n",
    "                )\n",
    "\n",
    "            fit_params_validated = _check_method_params(X, params=fit_params)\n",
    "            routed_params = Bunch(estimator=Bunch(fit=fit_params_validated))\n",
    "            if sample_weight is not None:\n",
    "                routed_params.estimator.fit[\"sample_weight\"] = sample_weight\n",
    "        X_new = []\n",
    "        self.custom_allow_cols = []\n",
    "        if custom_allow_cols:\n",
    "            for list_cols in custom_allow_cols[:y.shape[1]]:\n",
    "                self.custom_allow_cols.append(list_cols)\n",
    "                X_new.append(X[list_cols])\n",
    "            for _ in range(y.shape[1]-len(custom_allow_cols)):\n",
    "                self.custom_allow_cols.append(list(X.columns))\n",
    "                X_new.append(X)\n",
    "\n",
    "        for _ in range(y.shape[1]-len(custom_set_params)):\n",
    "            custom_set_params.append({})\n",
    "        # print(routed_params.estimator.fit)\n",
    "        if 'custom_allow_cols' in routed_params.estimator.fit.keys():\n",
    "            routed_params.estimator.fit.pop('custom_allow_cols')\n",
    "        if 'custom_set_params' in routed_params.estimator.fit.keys():\n",
    "            routed_params.estimator.fit.pop('custom_set_params')\n",
    "        if X_new:\n",
    "            self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
    "                delayed(_fit_estimator)(\n",
    "                    self.estimator, X_new[i], y[:, i], set_params=custom_set_params[i], **routed_params.estimator.fit\n",
    "                )\n",
    "                for i in range(y.shape[1])\n",
    "            )\n",
    "        else:\n",
    "            self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
    "                delayed(_fit_estimator)(\n",
    "                    self.estimator, X, y[:, i], set_params=custom_set_params[i], **routed_params.estimator.fit\n",
    "                )\n",
    "                for i in range(y.shape[1])\n",
    "            )\n",
    "\n",
    "        if hasattr(self.estimators_[0], \"n_features_in_\"):\n",
    "            self.n_features_in_ = self.estimators_[0].n_features_in_\n",
    "        if hasattr(self.estimators_[0], \"feature_names_in_\"):\n",
    "            self.feature_names_in_ = self.estimators_[0].feature_names_in_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict multi-output variable using model for each target variable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : {array-like, sparse matrix} of shape (n_samples, n_outputs)\n",
    "            Multi-output targets predicted across multiple predictors.\n",
    "            Note: Separate models are generated for each predictor.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        if not hasattr(self.estimators_[0], \"predict\"):\n",
    "            raise ValueError(\"The base estimator should implement a predict method\")\n",
    "        if not self.custom_allow_cols:\n",
    "            y = sk_Parallel(n_jobs=self.n_jobs)(\n",
    "                sk_delayed(e.predict)(X) for e in self.estimators_\n",
    "            )\n",
    "        else:\n",
    "            y = sk_Parallel(n_jobs=self.n_jobs)(\n",
    "                sk_delayed(e.predict)(X[self.custom_allow_cols[i]]) for i, e in enumerate(self.estimators_)\n",
    "            )\n",
    "        return np.asarray(y).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07db717",
   "metadata": {},
   "source": [
    "`train_fold()`: Đây là hàm tạo một fold duy nhất trong k-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6abe361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(fold, n_jobs, train_index, val_index,\n",
    "               feature_train, target_train, feature_test, weights_train, best_hypr, combined, model_type='lgb',\n",
    "               custom_set_params=[], custom_allow_cols=[],\n",
    "               skip_opt=False,\n",
    "               focus_fold=None, save_end=True):\n",
    "    \"\"\"\n",
    "    Train a single fold.\n",
    "    \"\"\"\n",
    "\n",
    "    if focus_fold is not None:\n",
    "        fold = focus_fold\n",
    "    print(f\"Training fold {fold + 1}...\")\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val = feature_train.iloc[train_index].copy(), feature_train.iloc[val_index].copy()\n",
    "    y_train, y_val = target_train.iloc[train_index].copy(), target_train.iloc[val_index].copy()\n",
    "    weight_train, weight_val = weights_train.iloc[train_index].copy(), weights_train.iloc[val_index].copy()\n",
    "\n",
    "    model_lgb_reg = LGBMRegressor(\n",
    "                            objective='binary',\n",
    "                            random_state=42,\n",
    "                            device_type='cpu',\n",
    "                          **best_hypr.copy(),\n",
    "                          # is_unbalance=True,\n",
    "                            extra_trees=True, # use for faster training with litte loss of accuracy\n",
    "                            # class_weight={0:1,1:1.5},\n",
    "                            verbose=-1,\n",
    "                          n_jobs=multiprocessing.cpu_count()//n_jobs,\n",
    "    )\n",
    "    model_lgb_clf = LGBMClassifier(\n",
    "                            objective='binary',\n",
    "                            random_state=42,\n",
    "                            device_type='cpu',\n",
    "                          **best_hypr.copy(),\n",
    "                          # is_unbalance=True,\n",
    "                            extra_trees=True,\n",
    "                            # class_weight={0:1,1:1.5},\n",
    "                            verbose=-1,\n",
    "                          n_jobs=multiprocessing.cpu_count()//n_jobs,\n",
    "    )\n",
    "\n",
    "    model_xgb_reg = XGBRegressor(objective='binary:logistic', **best_hypr.copy(),\n",
    "                      device='cpu',\n",
    "                      # weight_column='weight',\n",
    "                      random_state=42,\n",
    "                      enable_categorical=True,\n",
    "                      nthread=multiprocessing.cpu_count()//n_jobs,\n",
    "                    )\n",
    "    model_xgb_clf = XGBClassifier(objective='binary:logistic', **best_hypr.copy(),\n",
    "                      device='cpu',\n",
    "                      random_state=42,\n",
    "                      enable_categorical=True,\n",
    "                      # nthread=multiprocessing.cpu_count()//n_jobs,\n",
    "                      nthread=multiprocessing.cpu_count()//n_jobs,\n",
    "                    )\n",
    "\n",
    "    if 'reg_alpha' in list(best_hypr.keys()):\n",
    "        best_hypr.pop('reg_alpha')\n",
    "    if 'max_leaves' in list(best_hypr.keys()):\n",
    "        best_hypr.pop('max_leaves')\n",
    "\n",
    "    model_cat_clf = CatBoostClassifier(loss_function=\"CrossEntropy\", **best_hypr.copy(), # MultiLogloss, MultiCrossEntropy\n",
    "        verbose=False,\n",
    "        random_seed=42,\n",
    "        thread_count=multiprocessing.cpu_count()//n_jobs,\n",
    "        grow_policy='Lossguide',\n",
    "        task_type = 'CPU',\n",
    "        cat_features=list(X_train.select_dtypes(include=['category']).columns),\n",
    "    )\n",
    "\n",
    "\n",
    "    if target_train.shape[-1]>1 or 1:\n",
    "        if model_type == 'lgb_reg':\n",
    "            model = CustomMultiOutputRegressor(model_lgb_reg)\n",
    "        elif model_type == 'lgb_clf':\n",
    "            model = CustomMultiOutputRegressor(model_lgb_clf)\n",
    "        elif model_type == 'xgb_reg':\n",
    "            model = CustomMultiOutputRegressor(model_xgb_reg)\n",
    "        elif model_type == 'xgb_clf':\n",
    "            model = CustomMultiOutputRegressor(model_xgb_clf)\n",
    "        elif model_type == 'cat_clf':\n",
    "            model = CustomMultiOutputRegressor(model_cat_clf)\n",
    "        else:\n",
    "            model = CustomMultiOutputRegressor(model)\n",
    "\n",
    "    # Train the model\n",
    "    if fold==0:\n",
    "        print('START TRAIN')\n",
    "\n",
    "\n",
    "    if model_type=='tabm':\n",
    "        model.fit(X_train.drop(columns=CONFIG.target_cols), y_train.values,\n",
    "                  X_val=X_val.drop(columns=CONFIG.target_cols), y_val=y_val.values[:,0], # pytabkit must have val to work\n",
    "                 )\n",
    "    else:\n",
    "        model.fit(X_train.drop(columns=CONFIG.target_cols), y_train.values,\n",
    "                  custom_allow_cols=custom_allow_cols, custom_set_params=custom_set_params,\n",
    "             )\n",
    "\n",
    "    if fold==0:\n",
    "        print('END TRAIN')\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_pred_train = model.predict(X_train.drop(columns=CONFIG.target_cols))\n",
    "\n",
    "    y_pred_val = model.predict(X_val.drop(columns=CONFIG.target_cols))\n",
    "\n",
    "    # Calculate f1 for validation set\n",
    "    F1_score_trains = []\n",
    "    F1_score_vals = []\n",
    "\n",
    "\n",
    "    for i, c in enumerate(CONFIG.target_cols):\n",
    "        F1_score_trains.append(F1(y_train.values[:,i], y_pred_train[:,i], CONFIG.th[c], weight=None))\n",
    "        F1_score_vals.append(F1(y_val.values[:,i], y_pred_val[:,i], CONFIG.th[c], weight=None))\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_test = model.predict(feature_test)\n",
    "\n",
    "    if not skip_opt:\n",
    "        sc = {}\n",
    "        ts = []\n",
    "        for idx, c in enumerate(CONFIG.target_cols):\n",
    "            ts.append(f\"{c} Mean: {y_pred_test.mean():4f}\".ljust(10))\n",
    "\n",
    "        print(f\"Fold {fold} val F1: {' '.join([*map(str, F1_score_vals)])}\".ljust(9), f\"Mean: {(y_pred_test).mean():.2f}\",\n",
    "                          f\"train F1: {' '.join([*map(str, F1_score_trains)])}\".ljust(9), ' '.join(ts)\n",
    "         )\n",
    "\n",
    "    feature_importances = []\n",
    "    lgbm_shaps = []\n",
    "    \n",
    "    for i in range(len(model.estimators_)):\n",
    "        if 'LGBM' in str(model):\n",
    "            lgbm_shaps.append(model.estimators_[i].predict(feature_test, pred_contrib=True))\n",
    "            feature_importances.append(pd.Series(model.estimators_[i].feature_importances_,\n",
    "                                                 index=X_train.drop(columns=CONFIG.target_cols).columns).sort_values(ascending=False))\n",
    "        elif \"XGB\" in str(model):\n",
    "            feature_importances.append(pd.Series(model.estimators_[i].get_booster().get_score(importance_type='gain'),\n",
    "                                                 index=X_train.drop(columns=CONFIG.target_cols).columns).sort_values(ascending=False))\n",
    "\n",
    "    os.makedirs('/kaggle/tmp/folds', exist_ok=True)\n",
    "    os.makedirs('/kaggle/working/folds', exist_ok=True)\n",
    "    if save_end:\n",
    "        for i in range(len(model.estimators_)):\n",
    "            model.estimators_[i].booster_.save_model(f'/kaggle/working/folds/model_{CONFIG.target_cols[i]}_{model_type}_fold_{fold}.txt')\n",
    "\n",
    "    return fold, y_pred_train, y_pred_val, y_pred_test, F1_score_trains, F1_score_vals, feature_importances, lgbm_shaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a1c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 8\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "splits = balanced_kfold_split(\n",
    "    feature_train,\n",
    "    df_merge,\n",
    "    n_splits=n_splits,\n",
    "    group_column='ADHD_Outcome',\n",
    "    # df_buff=df_merge[(df_merge['Sex_F']==1) & (df_merge['ADHD_Outcome']==1)]\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "splits = [[feature_train.index, feature_train.index]]\n",
    "\n",
    "n_splits = len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5977d79c",
   "metadata": {},
   "source": [
    "`best_hypr_dict` is obtained via running `optim.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee97806",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hypr_dict = {\n",
    "'ADHD_Outcome':{\n",
    "    # lgb 1 fold\n",
    "    'lgb_reg_1' : [\n",
    "        {'reg_alpha': 4.455169907706503e-08, 'reg_lambda': 0.0012155452141569558, 'learning_rate': 0.01, 'n_estimators': 200, 'max_leaves': 118, 'min_child_samples': 9},\n",
    "        {'reg_alpha': 0.0014322603999347813, 'reg_lambda': 0.017887504299023544, 'learning_rate': 0.01, 'n_estimators': 500, 'max_leaves': 107, 'min_child_samples': 40}\n",
    "    ],\n",
    "    # lgb 8 folds\n",
    "    'lgb_reg_8':[\n",
    "        {'reg_alpha': 7.21654762517268e-07, 'reg_lambda': 1.7302178976767463e-06, 'learning_rate': 0.01, 'n_estimators': 200, 'max_leaves': 77, 'min_child_samples': 14},\n",
    "        {'reg_alpha': 0.0001545532323089221, 'reg_lambda': 2.353979413723042e-05, 'learning_rate': 0.01, 'n_estimators': 200, 'max_leaves': 130, 'min_child_samples': 19},\n",
    "    ],\n",
    "    # lgbclf 1 fold\n",
    "    # lgbclf 8 folds\n",
    "    'lgb_clf_8': [\n",
    "        {'reg_alpha': 6.057278408593891e-05, 'reg_lambda': 0.001249829196522666, 'learning_rate': 0.04, 'n_estimators': 200, 'max_leaves': 15, 'min_child_samples': 9},\n",
    "    ],\n",
    "\n",
    "    # xgb 1 folds\n",
    "    'xgb_reg_1':[\n",
    "        {'reg_alpha': 0.029033820445384596, 'reg_lambda': 4.74054550093646, 'learning_rate': 0.01, 'n_estimators': 227, 'max_depth': 7, 'max_leaves': 72},\n",
    "        {'reg_alpha': 1.3172799701422377, 'reg_lambda': 0.0007695940253933907, 'learning_rate': 0.01, 'n_estimators': 213, 'max_depth': 9, 'max_leaves': 28},\n",
    "        {'reg_alpha': 1.4521971699116607, 'reg_lambda': 1.489761594015251e-05, 'learning_rate': 0.01, 'n_estimators': 234, 'max_depth': 5, 'max_leaves': 74},\n",
    "        {'reg_alpha': 1.3369587538390495, 'reg_lambda': 1.0065560753084458e-08, 'learning_rate': 0.01, 'n_estimators': 197, 'max_depth': 5, 'max_leaves': 109},\n",
    "        {'reg_alpha': 1.1759442280666481, 'reg_lambda': 0.00044596125226314574, 'learning_rate': 0.01, 'n_estimators': 193, 'max_depth': 8, 'max_leaves': 28},\n",
    "        {'reg_alpha': 1.361667961454591, 'reg_lambda': 4.459283632001179e-06, 'learning_rate': 0.01, 'n_estimators': 200, 'max_depth': 5, 'max_leaves': 73},\n",
    "        {'reg_alpha': 3.1875098760107643, 'reg_lambda': 1.4252354685374041e-05, 'learning_rate': 0.01, 'n_estimators': 328, 'max_depth': 6, 'max_leaves': 70},\n",
    "        {'reg_alpha': 1.8518674080021942, 'reg_lambda': 0.11597077081822346, 'learning_rate': 0.01, 'n_estimators': 272, 'max_depth': 7, 'max_leaves': 121},\n",
    "    ],\n",
    "    # xgb 8 folds\n",
    "    'xgb_reg_8': [\n",
    "        {'reg_alpha': 9.939092939542823, 'reg_lambda': 0.03321335519659749, 'learning_rate': 0.01, 'n_estimators': 104, 'max_depth': 5, 'max_leaves': 110}\n",
    "    ],\n",
    "    # xgbclf 8 folds\n",
    "    'xgb_clf_8':[\n",
    "        {'reg_alpha': 7.34563570994724e-08, 'reg_lambda': 0.0003272206976903343, 'learning_rate': 0.01, 'n_estimators': 228, 'max_depth': 8, 'max_leaves': 130},\n",
    "    ],\n",
    "},\n",
    "'Sex_F':{\n",
    "    # lgb 1 fold\n",
    "\n",
    "    'lgb_reg_1': [\n",
    "        {'reg_alpha': 1.246942210779823, 'reg_lambda': 2.5155216944449413e-06, 'learning_rate': 0.09, 'n_estimators': 300, 'max_leaves': 33, 'min_child_samples': 34},\n",
    "        {'reg_alpha': 5.35780081242787, 'reg_lambda': 0.007195987127870962, 'learning_rate': 0.09999999999999999, 'n_estimators': 500, 'max_leaves': 124, 'min_child_samples': 45},\n",
    "        {'reg_alpha': 2.30724102951642, 'reg_lambda': 1.576755394055527e-07, 'learning_rate': 0.08, 'n_estimators': 400, 'max_leaves': 62, 'min_child_samples': 29},\n",
    "        {'reg_alpha': 4.868930058080973, 'reg_lambda': 3.2388939558136083, 'learning_rate': 0.08, 'n_estimators': 400, 'max_leaves': 53, 'min_child_samples': 42},\n",
    "        {'reg_alpha': 4.258789560980834, 'reg_lambda': 0.00017535261659203767, 'learning_rate': 0.09, 'n_estimators': 500, 'max_leaves': 119, 'min_child_samples': 42},\n",
    "        {'reg_alpha': 0.9502602345156709, 'reg_lambda': 0.01681412200856409, 'learning_rate': 0.060000000000000005, 'n_estimators': 200, 'max_leaves': 37, 'min_child_samples': 44},\n",
    "    ],\n",
    "    # # lgb 8 fold\n",
    "    'lgb_reg_8': [\n",
    "        {'reg_alpha': 1.7116594317226732, 'reg_lambda': 5.697803992056026, 'learning_rate': 0.08, 'n_estimators': 400, 'max_leaves': 29, 'min_child_samples': 38},\n",
    "        {'reg_alpha': 1.0097998663971504, 'reg_lambda': 2.1071847869742293e-05, 'learning_rate': 0.08, 'n_estimators': 500, 'max_leaves': 15, 'min_child_samples': 41},\n",
    "        {'reg_alpha': 2.3257113562606175, 'reg_lambda': 5.738586992184839, 'learning_rate': 0.08, 'n_estimators': 500, 'max_leaves': 53, 'min_child_samples': 39},\n",
    "        {'reg_alpha': 3.1347560659062945, 'reg_lambda': 7.079193366751104, 'learning_rate': 0.08, 'n_estimators': 500, 'max_leaves': 32, 'min_child_samples': 37},\n",
    "    ],\n",
    "    # xgb 1 fold\n",
    "    'xgb_reg_1': [\n",
    "        {'reg_alpha': 0.3648559885746107, 'reg_lambda': 2.59285720838081e-07, 'learning_rate': 0.09, 'n_estimators': 270, 'max_depth': 9, 'max_leaves': 21},\n",
    "    ],\n",
    "},\n",
    "}\n",
    "\n",
    "submit_df = pd.read_excel('data/SAMPLE_SUBMISSION.xlsx')\n",
    "\n",
    "ft_imps = {'ADHD_Outcome': [], 'Sex_F': []}\n",
    "lgbm_shap_scores = {'ADHD_Outcome': [], 'Sex_F': []}\n",
    "\n",
    "for target in ['ADHD_Outcome', 'Sex_F']:\n",
    "    # for target in ['ADHD_Outcome']:\n",
    "    # for target in ['Sex_F']:\n",
    "    print(f\"START TRAINING TARGET: {target}\")\n",
    "    CONFIG.target_cols = [target]\n",
    "    feature_train, target_train, weights_train, feature_test = get_data(df_merge, test_merge, save_out=False)\n",
    "\n",
    "    # We don't use the functional connectome features from ADHD predict!\n",
    "    if target=='ADHD_Outcome':\n",
    "        feature_train = feature_train.drop([col for col in feature_test.columns if 'throw' in col], axis=1)\n",
    "        feature_test = feature_test.drop([col for col in feature_test.columns if 'throw' in col], axis=1)\n",
    "\n",
    "    oof_all = np.zeros((len(feature_test), target_train.values.shape[-1]))\n",
    "    oof_len = 0\n",
    "    for k in tqdm(best_hypr_dict[target].keys()):\n",
    "        model_type, n_folds = k.rsplit('_', maxsplit=1)\n",
    "        n_folds = int(n_folds)\n",
    "        splits = balanced_kfold_split(feature_train, df_merge, n_splits=n_folds,\n",
    "                              group_column='ADHD_Outcome',\n",
    "                              seed=42,\n",
    "                             )\n",
    "\n",
    "        n_splits = len(splits)\n",
    "\n",
    "        # Run training for each hyperparameter set\n",
    "        for best_hypr in best_hypr_dict[target][k]:\n",
    "\n",
    "            n_jobs_x = min(8, multiprocessing.cpu_count()//4) # ensure we have at least 4 cores for each fold\n",
    "\n",
    "            results = Parallel(n_jobs=n_jobs_x, backend='threading')(\n",
    "                delayed(train_fold)(fold, min(len(splits), n_jobs_x), train_index, val_index,\n",
    "                                    feature_train.copy(deep=False), target_train.copy(), feature_test.copy(), weights_train,\n",
    "                                    deepcopy(best_hypr), combined.copy(deep=False),\n",
    "                                    custom_allow_cols = [],\n",
    "                                    model_type=model_type,\n",
    "                                    skip_opt=False,\n",
    "                                    save_end=0\n",
    "                                   )\n",
    "                for fold, (train_index, val_index) in tqdm(enumerate(splits), total=len(splits))\n",
    "            )\n",
    "\n",
    "            # Sort results to prevent origin order\n",
    "            results = sorted(results, key=lambda x: x[0])\n",
    "            \n",
    "            oof_len += n_splits\n",
    "\n",
    "            f1_scores_train = []\n",
    "            f1_scores = []\n",
    "\n",
    "            y_pred_vals = []\n",
    "            \n",
    "            for i, fold_result in enumerate(results):\n",
    "                idx, y_pred_train_, y_pred_val_, y_pred_test_, f1_score_train, F1_score, feature_importances, lgbm_shaps = fold_result\n",
    "\n",
    "                oof_all += y_pred_test_\n",
    "                y_pred_vals.append(y_pred_val_)\n",
    "                ft_imps[target].append(feature_importances)\n",
    "                lgbm_shap_scores[target].append(lgbm_shaps)\n",
    "                f1_scores_train.append(f1_score_train)\n",
    "                f1_scores.append(F1_score)\n",
    "\n",
    "\n",
    "    # Final predictions: average the accumulated predictions\n",
    "    final_predictions_proba = oof_all / oof_len\n",
    "    final_predictions = np.concatenate([(final_predictions_proba[:,i] > CONFIG.th[tgx]).astype(int)[:, None] for i, tgx in enumerate(CONFIG.target_cols)], axis=1)\n",
    "\n",
    "    print(f\"Final predictions mean: {final_predictions.mean(axis=0)}\")\n",
    "    print(f\"Amount 1 in predictions:  {final_predictions.sum(axis=0)}\")\n",
    "\n",
    "    for i, c in enumerate(CONFIG.target_cols):\n",
    "        final_predictions[:,i] = (final_predictions_proba[:,i] > CONFIG.th[c]).astype(int)\n",
    "        submit_df[c] = final_predictions[:,i]\n",
    "\n",
    "print('Total runtime:', time.time()-start_time)\n",
    "submit_df.to_csv('data/submission.csv', index=False)\n",
    "submit_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064ef4fc",
   "metadata": {},
   "source": [
    "# 5. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6472d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check feature important\n",
    "feature_importances = ft_imps['ADHD_Outcome'][0][0]\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "palette = sns.color_palette(\"RdYlGn_r\", len(feature_train.columns))\n",
    "# lgbm_importances = pd.Series(model.feature_importances_, index=feature_train.columns).sort_values(ascending=False)\n",
    "sns.barplot(y=feature_importances.index, x=feature_importances.values, orient='h', palette=palette)\n",
    "\n",
    "plt.title('ADHD_Outcome Model Feature Important')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da203c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check feature important\n",
    "feature_importances = ft_imps['Sex_F'][0][0][:60]\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "palette = sns.color_palette(\"RdYlGn_r\", len(feature_train.columns))\n",
    "sns.barplot(y=feature_importances.index, x=feature_importances.values, orient='h', palette=palette)\n",
    "\n",
    "plt.title('Sex_F Model Feature Important')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
