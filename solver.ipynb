{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a1229d",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df13641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\", font_scale=1.5)\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10932863",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q = pd.read_excel(\"data/TRAIN/TRAIN_QUANTITATIVE_METADATA.xlsx\")\n",
    "train_c = pd.read_excel(\"data/TRAIN/TRAIN_CATEGORICAL_METADATA.xlsx\")\n",
    "test_q = pd.read_excel(\"data/TEST/TEST_QUANTITATIVE_METADATA.xlsx\")\n",
    "test_c = pd.read_excel(\"data/TEST/TEST_CATEGORICAL_METADATA.xlsx\")\n",
    "\n",
    "train_combined = pd.merge(train_q, train_c, on='participant_id', how='left').set_index(\"participant_id\")\n",
    "test_combined = pd.merge(test_q, test_c, on='participant_id', how='left').set_index(\"participant_id\")\n",
    "\n",
    "labels = pd.read_excel(\"data/TRAIN/TRAINING_SOLUTIONS.xlsx\").set_index('participant_id')\n",
    "\n",
    "y_adhd = labels['ADHD_Outcome']\n",
    "y_sex = labels['Sex_F']\n",
    "\n",
    "train_combined = train_combined.sort_index()\n",
    "test_combined = test_combined.sort_index()\n",
    "labels = labels.sort_index()\n",
    "assert all(train_combined.index == labels.index), \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8952f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6073c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_combined.columns)\n",
    "print(len(train_combined.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff3efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = labels.copy()\n",
    "\n",
    "# Plot: ADHD_Outcome\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# sns.countplot(x=label_df[\"ADHD_Outcome\"], palette=\"Set2\")\n",
    "# plt.title(\"Distribution of ADHD_Outcome\")\n",
    "# plt.xticks([0, 1], ['No ADHD (0)', 'ADHD (1)'])\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot: Sex_F\n",
    "# plt.figure(figsize=(6, 4))\n",
    "# sns.countplot(x=label_df[\"Sex_F\"], palette=\"pastel\")\n",
    "# plt.title(\"Distribution of Sex_F\")\n",
    "# plt.xticks([0, 1], ['Male (0)', 'Female (1)'])\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae4e623",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing\n",
    "This is some explanation of feature present in the dataset\n",
    "\n",
    "## 2.1 Numerical features: Total of 18 features\n",
    "+ EHQ_EHQ_Total: laterality index score (float) || -100 = 10th left, −28 ≤ LI < 48 = middle, 100 = 10th right\"\n",
    "+ ColorVision_CV_Score: color vision test score (int)\n",
    "+ MRI_Track_Age_at_Scan: Age at time of MRI scan (float)\n",
    "\n",
    "### ALABAMA PARENTING QUESTIONAIRE - PARENT REPORT (INT)\n",
    "+ APQ_P_APQ_P_CP: Reflects the frequency or severity of corporal punishment used by parents\n",
    "+ APQ_P_APQ_P_ID: Measures inconsistency in parental discipline\n",
    "+ APQ_P_APQ_P_INV: Indicates the level of parental involvement in the child’s life\n",
    "+ APQ_P_APQ_P_OPD: Other Discipline Practices Score (Not factored into total score but provides item level information)\n",
    "+ APQ_P_APQ_P_PM: Reflects how well a parent monitors and supervises their child\n",
    "+ APQ_P_APQ_P_PP: Captures the extent of positive reinforcement and supportive parenting\n",
    "\n",
    "### Strength and Difficulties Questionnaire (INT)\n",
    "+ SDQ_SDQ_Conduct_Problems: Measures behavioral issues related to rule-breaking or aggression (higher score = more prone to ADHD)\n",
    "+ SDQ_SDQ_Difficulties_Total: A composite measure summarizing overall difficulties across several behavioral domains\n",
    "+ SDQ_SDQ_Emotional_Problems: Focuses on internal emotional difficulties such as anxiety or depression (social related)\n",
    "+ SDQ_SDQ_Externalizing: Captures outward-directed behaviors such as hyperactivity, impulsivity, and conduct issues\n",
    "+ SDQ_SDQ_Generating_Impact: This might reflect the overall impact of the child’s behavioral problems on their social and academic life\n",
    "+ SDQ_SDQ_Hyperactivity: Directly measures the hyperactive and impulsive behaviors central to many ADHD diagnoses (HIGHLY CORRELATED FEATURE)\n",
    "+ SDQ_SDQ_Internalizing: Reflects inward-focused behaviors such as social withdrawal and anxiety\n",
    "+ SDQ_SDQ_Peer_Problems: Assesses difficulties in interacting with peers\n",
    "+ SDQ_SDQ_Prosocial: Evaluates positive social behaviors like empathy and cooperation\n",
    "\n",
    "## 2.2 Categorical Features Visualization: Total of 10 features (already label encoded)\n",
    "\n",
    "+ Basic_Demos_Enroll_Year: the year when the participant enrolled in the study (int) (Nomial)\n",
    "+ Basic_Demos_Study_Site: Location/site where the subject was assessed (Nomial)\n",
    "+ PreInt_Demos_Fam_Child_Ethnicity: Ethnic background of the child (nomial)\n",
    "+ PreInt_Demos_Fam_Child_Race: Race of the child (nomial)\n",
    "+ MRI_Track_Scan_Location: Where the MRI was performed (nomial)\n",
    "+ Barratt_Barratt_P1_Edu: education of the parent 1 (ORDINAL)\n",
    "+ Barratt_Barratt_P1_Occ: occupation of parent 1 (ORDINAL)\n",
    "+ Barratt_Barratt_P2_Edu: education of the parent 2 (ORDINAL)\n",
    "+ Barratt_Barratt_P2_Occ: occupation of parent 2 (ORDINAL)\n",
    "+ Laterality_Category: Categorical brain lateralization: left, middle, or right\n",
    "\n",
    "## 2.3 fMRI Connectome Matrices\n",
    "+ Dimensionality Reduction: Apply techniques like Principal Component Analysis (PCA), Independent Component Analysis (ICA), or Uniform Manifold Approximation and Projection (UMAP) to the flattened connectome matrices before applying other feature selection methods or feeding them into models like Logistic Regression or standard Tree-based algorithms. Deep Learning models might handle the high dimensionality better directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f4282",
   "metadata": {},
   "source": [
    "Firstly, we deal with numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75927b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    'EHQ_EHQ_Total', 'ColorVision_CV_Score', 'APQ_P_APQ_P_CP',\n",
    "    'APQ_P_APQ_P_ID', 'APQ_P_APQ_P_INV', 'APQ_P_APQ_P_OPD',\n",
    "    'APQ_P_APQ_P_PM', 'APQ_P_APQ_P_PP', 'SDQ_SDQ_Conduct_Problems',\n",
    "    'SDQ_SDQ_Difficulties_Total', 'SDQ_SDQ_Emotional_Problems',\n",
    "    'SDQ_SDQ_Externalizing', 'SDQ_SDQ_Generating_Impact',\n",
    "    'SDQ_SDQ_Hyperactivity', 'SDQ_SDQ_Internalizing',\n",
    "    'SDQ_SDQ_Peer_Problems', 'SDQ_SDQ_Prosocial', 'MRI_Track_Age_at_Scan'\n",
    "]\n",
    "print(len(numerical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651916ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features_df = train_combined[numerical_features]\n",
    "\n",
    "def nan_summary(df, name):\n",
    "    print(f\"---- {name} NAN SUMMARY ----\")\n",
    "    print((df.isnull().sum() / len(df) * 100).sort_values(ascending=False), '\\n')\n",
    "\n",
    "nan_summary(numerical_features_df, \"TRAIN NUMERICAL FEATURES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c4a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num feature 1 \n",
    "feature_1 = 'EHQ_EHQ_Total'\n",
    "\n",
    "median_value = train_combined[feature_1].median()\n",
    "train_combined[feature_1] = train_combined[feature_1].fillna(median_value)\n",
    "print(f\"Filled NaN with median value: {median_value}\")\n",
    "\n",
    "median_test_value = test_combined[feature_1].median()\n",
    "test_combined[feature_1] = test_combined[feature_1].fillna(median_test_value)\n",
    "print(f\"Filled NaN with median value: {median_test_value}\")\n",
    "\n",
    "def categorize_laterality(li):\n",
    "    if -110 <= li <= -28:\n",
    "        return \"Left-Lateralized\"\n",
    "    elif -28 < li <= 47:\n",
    "        return 'Middle'\n",
    "    elif 47 < li <= 110:\n",
    "        return 'Right-Lateralized'\n",
    "\n",
    "train_combined['Laterality_Category'] = train_combined['EHQ_EHQ_Total'].apply(categorize_laterality)\n",
    "test_combined['Laterality_Category'] = test_combined['EHQ_EHQ_Total'].apply(categorize_laterality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num feature 2\n",
    "train_combined['ColorVision_CV_Score'].fillna(14, inplace=True)\n",
    "print(\"Missing values after fill:\", train_combined[\"ColorVision_CV_Score\"].isna().sum())\n",
    "\n",
    "test_combined['ColorVision_CV_Score'].fillna(14, inplace=True)\n",
    "print(\"Missing values after fill:\", test_combined[\"ColorVision_CV_Score\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d695b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "apq_features = [\n",
    "    'APQ_P_APQ_P_CP', 'APQ_P_APQ_P_ID', 'APQ_P_APQ_P_INV', \n",
    "    'APQ_P_APQ_P_OPD', 'APQ_P_APQ_P_PM', 'APQ_P_APQ_P_PP', \n",
    "]\n",
    "\n",
    "for feature in apq_features:\n",
    "    median_value = train_combined[feature].median()\n",
    "    train_combined[feature] = train_combined[feature].fillna(median_value)\n",
    "    print(f\"{feature} - Filled train NaN with median: {median_value}\")\n",
    "    \n",
    "    # For test dataset\n",
    "    median_test_value = test_combined[feature].median()\n",
    "    test_combined[feature] = test_combined[feature].fillna(median_test_value)\n",
    "    print(f\"{feature} - Filled test NaN with median: {median_test_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a4baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDQ features\n",
    "sdq_features = [\n",
    "    'SDQ_SDQ_Conduct_Problems', 'SDQ_SDQ_Emotional_Problems', \n",
    "    'SDQ_SDQ_Externalizing', 'SDQ_SDQ_Generating_Impact',\n",
    "    'SDQ_SDQ_Hyperactivity', 'SDQ_SDQ_Internalizing',\n",
    "    'SDQ_SDQ_Peer_Problems', 'SDQ_SDQ_Prosocial',\n",
    "    'SDQ_SDQ_Difficulties_Total'\n",
    "]\n",
    "\n",
    "for feature in sdq_features:\n",
    "    median_value = train_combined[feature].median()\n",
    "    train_combined[feature] = train_combined[feature].fillna(median_value)\n",
    "    print(f\"{feature} - Filled train NaN with median: {median_value}\")\n",
    "    \n",
    "    median_test_value = test_combined[feature].median()\n",
    "    test_combined[feature] = test_combined[feature].fillna(median_test_value)\n",
    "    print(f\"{feature} - Filled test NaN with median: {median_test_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2029c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = train_combined[numerical_features].copy()\n",
    "imputer = IterativeImputer(estimator=LassoCV(random_state=SEED), max_iter=100, random_state=SEED)\n",
    "\n",
    "numeric_data_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(numeric_data), \n",
    "    columns=numerical_features, \n",
    "    index=numeric_data.index\n",
    ")\n",
    "train_combined['MRI_Track_Age_at_Scan'] = numeric_data_imputed['MRI_Track_Age_at_Scan']\n",
    "\n",
    "\n",
    "test_numeric_data = test_combined[numerical_features].copy()\n",
    "test_numeric_data_imputed = pd.DataFrame(\n",
    "    imputer.transform(test_numeric_data),\n",
    "    columns=numerical_features,\n",
    "    index=test_numeric_data.index\n",
    ")\n",
    "test_combined['MRI_Track_Age_at_Scan'] = test_numeric_data_imputed['MRI_Track_Age_at_Scan']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88138f43",
   "metadata": {},
   "source": [
    "## Now we handle categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f18d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    'Basic_Demos_Enroll_Year', 'Basic_Demos_Study_Site',\n",
    "    'PreInt_Demos_Fam_Child_Ethnicity', 'PreInt_Demos_Fam_Child_Race',\n",
    "    'MRI_Track_Scan_Location', 'Barratt_Barratt_P1_Edu',\n",
    "    'Barratt_Barratt_P1_Occ', 'Barratt_Barratt_P2_Edu',\n",
    "    'Barratt_Barratt_P2_Occ', 'Laterality_Category',\n",
    "]\n",
    "\n",
    "print(len(categorical_features))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c7e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_df = train_combined[categorical_features]\n",
    "\n",
    "nan_summary(categorical_features_df, \"TRAIN CATEGORICAL FEATURES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in categorical_features:\n",
    "    if feature in train_combined.columns:\n",
    "        unique_values = train_combined[feature].unique()\n",
    "        print(f\"\\nFeature: '{feature}'\")\n",
    "        print(f\"Unique values: {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1259f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1 = 'Basic_Demos_Enroll_Year'\n",
    "\n",
    "threshold = 50\n",
    "\n",
    "category_counts = train_combined[feature_1].value_counts()\n",
    "rare_categories = category_counts[category_counts < threshold].index\n",
    "train_combined[feature_1] = train_combined[feature_1].apply(\n",
    "    lambda x: 'Other' if x in rare_categories else str(x)\n",
    ")\n",
    "\n",
    "test_combined[feature_1] = test_combined[feature_1].astype(str)\n",
    "allowed_categories = set(train_combined[feature_1].unique())\n",
    "\n",
    "test_combined[feature_1] = test_combined[feature_1].apply(\n",
    "    lambda x: x if x in allowed_categories else 'Other'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.countplot(data=train_combined, x='Basic_Demos_Enroll_Year', palette='Set2')\n",
    "plt.title(f\"Distribution of {feature_1}\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3217bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could collapse into only 2 classes \n",
    "feature_2 = 'Basic_Demos_Study_Site'\n",
    "train_combined[feature_2] = train_combined[feature_2].replace({3: 2, 4: 2})\n",
    "test_combined[feature_2] = test_combined[feature_2].replace({3: 2, 4: 2})\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=train_combined, x='Basic_Demos_Study_Site', palette='Set2')\n",
    "plt.title(f\"Distribution of {feature_2}\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a42c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_3 = \"PreInt_Demos_Fam_Child_Ethnicity\"\n",
    "train_combined[feature_3] = train_combined[feature_3].fillna(3.0)\n",
    "test_combined[feature_3] = test_combined[feature_3].fillna(3.0)\n",
    "\n",
    "train_combined[feature_3] = train_combined[feature_3].replace({3: 2})\n",
    "test_combined[feature_3] = test_combined[feature_3].replace({3: 2})\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=train_combined, x='PreInt_Demos_Fam_Child_Ethnicity', palette='Set2')\n",
    "plt.title(f\"Distribution of {feature_3}\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ac8f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_4 = 'PreInt_Demos_Fam_Child_Race'\n",
    "train_combined[feature_4] = train_combined[feature_4].fillna(10.0)\n",
    "test_combined[feature_4] = test_combined[feature_4].fillna(10.0)\n",
    "\n",
    "train_combined[feature_4] = train_combined[feature_4].apply(\n",
    "    lambda x: \"White\" if x == 0.0 else \"Non-White\"\n",
    ")\n",
    "\n",
    "test_combined[feature_4] = test_combined[feature_4].apply(\n",
    "    lambda x: 'White' if x == 0.0 else 'Non-White'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=train_combined, x='PreInt_Demos_Fam_Child_Race', palette='Set2')\n",
    "plt.title(f\"Distribution of {feature_4}\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb9fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_5 = \"MRI_Track_Scan_Location\"\n",
    "train_combined[feature_5] = train_combined[feature_5].fillna(4.0)   \n",
    "test_combined[feature_5] = test_combined[feature_5].fillna(4.0)\n",
    "\n",
    "train_combined[feature_5] = train_combined[feature_5].replace({4: 1})\n",
    "test_combined[feature_5] = test_combined[feature_5].replace({4: 1})\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=train_combined, x='MRI_Track_Scan_Location', palette='Set2')\n",
    "plt.title(f\"Distribution of {feature_5}\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_6 = \"Barratt_Barratt_P1_Edu\"\n",
    "train_combined[feature_6] = train_combined[feature_6].fillna(18.0)\n",
    "test_combined[feature_6] = test_combined[feature_6].fillna(18.0)\n",
    "\n",
    "def collapse_edu(value):\n",
    "    if value == 21.0:\n",
    "        return \"Upper_College\"\n",
    "    elif value == 18.0:\n",
    "        return \"College\"\n",
    "    else:\n",
    "        return \"Under_College\"\n",
    "    \n",
    "train_combined[feature_6] = train_combined[feature_6].apply(collapse_edu)\n",
    "test_combined[feature_6] = test_combined[feature_6].apply(collapse_edu)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=train_combined, x=\"Barratt_Barratt_P1_Edu\", palette='Set2')\n",
    "plt.title(f\"Distribution of {feature_6}\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec76c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_7 = \"Barratt_Barratt_P1_Occ\"\n",
    "\n",
    "median_train_value = train_combined[feature_7].median()\n",
    "median_test_value = test_combined[feature_7].median()\n",
    "\n",
    "train_combined[feature_7] = train_combined[feature_7].fillna(median_train_value)\n",
    "test_combined[feature_7] = test_combined[feature_7].fillna(median_test_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7252f904",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_8 = \"Barratt_Barratt_P2_Edu\"\n",
    "\n",
    "train_combined[feature_8] = train_combined[feature_8].apply(collapse_edu)\n",
    "test_combined[feature_8] = test_combined[feature_8].apply(collapse_edu)\n",
    "\n",
    "def impute_p2_edu(row):\n",
    "    if pd.isna(row['Barratt_Barratt_P2_Edu']):\n",
    "        return row['Barratt_Barratt_P1_Edu']\n",
    "    return row['Barratt_Barratt_P2_Edu']\n",
    "\n",
    "train_combined['Barratt_Barratt_P2_Edu'] = train_combined.apply(impute_p2_edu, axis=1)\n",
    "test_combined['Barratt_Barratt_P2_Edu'] = test_combined.apply(impute_p2_edu, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=train_combined, x=\"Barratt_Barratt_P2_Edu\", palette='Set2')\n",
    "plt.title(f\"Distribution of {feature_8}\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_9 = \"Barratt_Barratt_P2_Occ\"\n",
    "\n",
    "def impute_p2_occ(row):\n",
    "    if pd.isna(row['Barratt_Barratt_P2_Occ']):\n",
    "        return row['Barratt_Barratt_P1_Occ']\n",
    "    return row['Barratt_Barratt_P2_Occ']\n",
    "\n",
    "train_combined[feature_9] = train_combined.apply(impute_p2_occ, axis=1)\n",
    "test_combined[feature_9] = test_combined.apply(impute_p2_occ, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15deb65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_occupation(val):\n",
    "    if val in [0.0, 5.0, 10.0, 15.0]:\n",
    "        return \"Low\"\n",
    "    elif val in [20.0, 25.0]:\n",
    "        return \"LowerMid\"\n",
    "    elif val in [30.0, 35.0, 40.0]:\n",
    "        return \"UpperMid\"\n",
    "    elif val == 45.0:\n",
    "        return \"High\"\n",
    "    else:\n",
    "        return \"Other\"  # Safety fallback\n",
    "    \n",
    "train_combined[feature_7] = train_combined[feature_7].apply(collapse_occupation)\n",
    "test_combined[feature_7] = test_combined[feature_7].apply(collapse_occupation)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=train_combined, x=\"Barratt_Barratt_P1_Occ\", palette='Set2')\n",
    "plt.title(f\"Distribution of {feature_7}\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c62d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined[feature_9] = train_combined[feature_9].apply(collapse_occupation)\n",
    "test_combined[feature_9] = test_combined[feature_9].apply(collapse_occupation)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=train_combined, x=\"Barratt_Barratt_P2_Occ\", palette='Set2')\n",
    "plt.title(f\"Distribution of {feature_9}\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efde69c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_encode_features = [\n",
    "#     'Basic_Demos_Enroll_Year',\n",
    "#     'Barratt_Barratt_P1_Edu',\n",
    "#     'Barratt_Barratt_P1_Occ',\n",
    "#     'Barratt_Barratt_P2_Edu',\n",
    "#     'Barratt_Barratt_P2_Occ',\n",
    "#     'Laterality_Category'\n",
    "# ]\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# encoders = {}  # Store encoders if needed for inverse_transform later\n",
    "\n",
    "# for feature in label_encode_features:\n",
    "#     le = LabelEncoder()\n",
    "#     train_combined[feature] = le.fit_transform(train_combined[feature])\n",
    "#     test_combined[feature] = le.transform(test_combined[feature])\n",
    "#     encoders[feature] = le  # Save encoder\n",
    "    \n",
    "# # Updated binary map: White = 1, Non-White = 0\n",
    "# binary_map = {\n",
    "#     'White': 1,\n",
    "#     'Non-White': 0,\n",
    "# }\n",
    "\n",
    "# # Apply to 'PreInt_Demos_Fam_Child_Race'\n",
    "# train_combined['PreInt_Demos_Fam_Child_Race'] = train_combined['PreInt_Demos_Fam_Child_Race'].map(binary_map)\n",
    "# test_combined['PreInt_Demos_Fam_Child_Race'] = test_combined['PreInt_Demos_Fam_Child_Race'].map(binary_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233018bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode nominal features\n",
    "nominal_cols = [\n",
    "    'Basic_Demos_Enroll_Year',\n",
    "    'Basic_Demos_Study_Site',\n",
    "    'PreInt_Demos_Fam_Child_Ethnicity',\n",
    "    'PreInt_Demos_Fam_Child_Race',\n",
    "    'MRI_Track_Scan_Location',\n",
    "    'Laterality_Category'\n",
    "]\n",
    "\n",
    "train_combined = pd.get_dummies(train_combined, columns=nominal_cols, drop_first=True, dtype=int)\n",
    "test_combined = pd.get_dummies(test_combined, columns=nominal_cols, drop_first=True, dtype=int)\n",
    "\n",
    "# Align columns\n",
    "train_combined, test_combined = train_combined.align(test_combined, join='left', axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c066faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_map = {\n",
    "    'Barratt_Barratt_P1_Edu': ['Under_College', 'College', 'Upper_College'],\n",
    "    'Barratt_Barratt_P2_Edu': ['Under_College', 'College', 'Upper_College'],\n",
    "    'Barratt_Barratt_P1_Occ': ['Low', 'LowerMid', 'UpperMid', 'High'],\n",
    "    'Barratt_Barratt_P2_Occ': ['Low', 'LowerMid', 'UpperMid', 'High']\n",
    "}\n",
    "\n",
    "ordinal_cols = list(ordinal_map.keys())\n",
    "categories = [ordinal_map[col] for col in ordinal_cols]\n",
    "\n",
    "encoder = OrdinalEncoder(categories=categories)\n",
    "\n",
    "train_combined[ordinal_cols] = encoder.fit_transform(train_combined[ordinal_cols])\n",
    "test_combined[ordinal_cols] = encoder.transform(test_combined[ordinal_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1607c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined.reset_index(inplace=True)\n",
    "test_combined.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10222b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined.to_csv(\"train_processed_ver2.csv\", index=False)\n",
    "test_combined.to_csv(\"test_processed_ver2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5f093",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering (Model-Agnostic)\n",
    "\n",
    "## Inter-feature correlation \n",
    "You might keep only one feature from a highly correlated group, perhaps the one more strongly correlated with the targets or based on domain knowledge. This is particularly important for Logistic Regression which is sensitive to multicollinearity.\n",
    "\n",
    "+ Covariance matrix: symmetric and positive semidefinite and tells us about the spread of the data => Plot the correlation matrix\n",
    "\n",
    "## Correlation with targets\n",
    "+ (Numerical features): The Pearson's correlation coefficient is often used, which is a normalized version of covariance that ranges from -1 to +1, providing a more standardized measure of the strength and direction of the linear relationship. This indicator only captures the linear relationship between variables. It is worth noting that when categorical features are encoded and they are NOMIAL, a low p-value might not reflect fully its influence\n",
    "Solution: non-linear transformation, interaction features, feature combinations (arithmetic), domain knowledge \n",
    "\n",
    "+ (Numerical and categorical features) Mutual information: Captures the statistical dependence between two random variables. Unlike correlation, MI can capture both linear and non-linear relationships. A higher MI score indicates a stronger dependency between the nominal feature and the categorical target.\n",
    "\n",
    "## Statistical Tests\n",
    "+ ANOVA F-test: For numerical features vs. each categorical target. Tests if the mean of the numerical feature differs significantly across the target groups (e.g., mean age for ADHD vs. non-ADHD). A significant p-value suggests relevance.\n",
    "\n",
    "+ Chi-Squared Test: For categorical features (like `PreInt_Demos_Fam_Child_Race`, `Laterality_Category`) vs. each categorical target. Tests for independence between the feature and the target. A significant p-value suggests dependence/relevance.\n",
    "\n",
    "## Solution\n",
    "Variance Threshold: Remove features with very low variance. These features are nearly constant and thus provide little predictive information. Be cautious with numerical features if they haven't been scaled.\n",
    "\n",
    "A feature might be considered important if it shows relevance (high correlation/MI, significant test) to either `ADHD_Outcome` or `Sex_F`. You can rank features based on their scores for each target and combine the rankings or set thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e1092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "from scipy.stats import pointbiserialr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Set visual style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "SEED  = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de2b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = pd.read_csv('train_processed_ver2.csv').set_index(\"participant_id\")\n",
    "test_metadata = pd.read_csv('test_processed_ver2.csv').set_index(\"participant_id\")\n",
    "\n",
    "labels = pd.read_excel(\"data/TRAIN/TRAINING_SOLUTIONS.xlsx\").set_index(\"participant_id\")\n",
    "\n",
    "train_combined = train_metadata.sort_index()\n",
    "test_combined = test_metadata   .sort_index()\n",
    "labels = labels.sort_index()\n",
    "\n",
    "assert all(train_combined.index == labels.index), \"Label IDs do not match train_combined IDs\"\n",
    "\n",
    "y_train_sex = labels['Sex_F']\n",
    "y_train_adhd = labels['ADHD_Outcome']\n",
    "\n",
    "print(\"Merged Training Data Shape:\", train_combined.shape)\n",
    "print(\"Merged Test Data Shape:\", test_combined.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e444a0",
   "metadata": {},
   "source": [
    "These are the correlation (normalised), p-value, and mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994de269",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_sex, p_sex = f_classif(train_combined, y_train_sex)\n",
    "mi_sex = mutual_info_classif(train_combined, y_train_sex, random_state=SEED)\n",
    "\n",
    "F_adhd, p_adhd = f_classif(train_combined, y_train_adhd)\n",
    "mi_adhd = mutual_info_classif(train_combined, y_train_adhd, random_state=SEED)\n",
    "\n",
    "corr_sex = []\n",
    "corr_adhd = []\n",
    "\n",
    "for feature in train_combined.columns:\n",
    "    try:\n",
    "        corr_s, _ = pointbiserialr(train_combined[feature], y_train_sex)\n",
    "        corr_a, _ = pointbiserialr(train_combined[feature], y_train_adhd)\n",
    "    except:\n",
    "        corr_s, corr_a = float('nan'), float('nan')\n",
    "    corr_sex.append(corr_s)\n",
    "    corr_adhd.append(corr_a)\n",
    "    \n",
    "    \n",
    "features = train_combined.columns\n",
    "\n",
    "df_sex = pd.DataFrame({\n",
    "    \"feature\": features,\n",
    "    \"MI\": mi_sex,\n",
    "    # \"F_stat\": F_sex,\n",
    "    \"p_value\": p_sex,\n",
    "    \"correlation\": [abs(i) for i in corr_sex]\n",
    "}).sort_values(by=\"p_value\", ascending=True)\n",
    "\n",
    "df_adhd = pd.DataFrame({\n",
    "    \"feature\": features,\n",
    "    \"MI\": mi_adhd,\n",
    "    # \"F_stat\": F_adhd,\n",
    "    \"p_value\": p_adhd,\n",
    "    \"correlation\": [abs(i) for i in corr_adhd]\n",
    "}).sort_values(by=\"p_value\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f94c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for features with p-value < 0.05 and abs(correlation) > 0.1\n",
    "df_sex_filtered = df_sex[(df_sex[\"p_value\"] < 0.05) & (df_sex[\"correlation\"].abs() > 0.05)]\n",
    "print(df_sex_filtered['feature'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b30ecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adhd_filtered = df_adhd[(df_adhd[\"p_value\"] < 0.05) & (df_adhd[\"correlation\"].abs() > 0.1)]\n",
    "print(df_adhd_filtered['feature'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab6d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "numerical_features = ['EHQ_EHQ_Total', 'ColorVision_CV_Score', 'APQ_P_APQ_P_CP',\n",
    "    'APQ_P_APQ_P_ID', 'APQ_P_APQ_P_INV', 'APQ_P_APQ_P_OPD',\n",
    "    'APQ_P_APQ_P_PM', 'APQ_P_APQ_P_PP', 'SDQ_SDQ_Conduct_Problems',\n",
    "    'SDQ_SDQ_Difficulties_Total', 'SDQ_SDQ_Emotional_Problems',\n",
    "    'SDQ_SDQ_Externalizing', 'SDQ_SDQ_Generating_Impact',\n",
    "    'SDQ_SDQ_Hyperactivity', 'SDQ_SDQ_Internalizing',\n",
    "    'SDQ_SDQ_Peer_Problems', 'SDQ_SDQ_Prosocial', 'MRI_Track_Age_at_Scan',\n",
    "]\n",
    "correlation_matrix = train_combined[numerical_features].corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed15fe6",
   "metadata": {},
   "source": [
    "# 3+: Feature Engineering (Model-Specific)\n",
    "\n",
    "+ Recursive Feature Elimination (RFE): Starts with all features, trains a model (e.g., Logistic Regression, SVM, or a Tree-based model), removes the least important feature(s) based on coefficients or feature importance scores, and repeats until the desired number of features is reached. The importance is evaluated based on the model's performance on a validation set using the weighted F1 score.\n",
    "\n",
    "+ Sequential Feature Selection (SFS): Forward Selection: Starts with no features, iteratively adds the feature that results in the best model performance (using the weighted F1 score) until no further improvement is seen.\n",
    "OR\n",
    "Backward Elimination: Starts with all features, iteratively removes the feature whose removal least degrades (or most improves) model performance (using the weighted F1 score).\n",
    "\n",
    "## Embedded Methods (Model-Integrated) \n",
    "+ L1 Regularization (Lasso): Used with linear models like Logistic Regression. Adds a penalty proportional to the absolute value of the coefficients. This forces some coefficients to become exactly zero, effectively1 removing those features from the model. You can train a Logistic Regression model with L1 penalty and select the features with non-zero coefficients.   \n",
    "\n",
    "+ Tree-Based Feature Importance: Models like Random Forest, Gradient Boosting Machines (XGBoost, LightGBM, CatBoost) naturally compute feature importance scores during training (e.g., based on Gini impurity reduction or the number of times a feature is used to split). Train a multi-output tree-based model and use these importance scores to rank and select features. Features with low importance can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51762f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa339c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b781e539",
   "metadata": {},
   "source": [
    "# 4. Modeling \n",
    "\n",
    "+ Logistic Regression: likely to converge to a point due to its simplistic architecture, despite how many feature engineering you did \n",
    "\n",
    "+ MLP: The depth of the network allows for hierarchical feature learning, which is particularly useful for complex non-linearities. Each layer the model learns new features, but it is a black box and we as humans cannot interpret what those features are\n",
    "\n",
    "+ Tree-based algorithm: Gradient Boosting Machines (GBM) (e.g., XGBoost, LightGBM, CatBoost): These are also ensemble methods that build trees sequentially, with each new tree trying to correct the errors made by the previous ones. They are highly effective at capturing intricate non-linear patterns and often achieve state-of-the-art performance\n",
    "\n",
    "+ Support Vector Machines with non-linear kernels: By using kernel functions (like Radial Basis Function (RBF), polynomial, or sigmoid), SVMs can implicitly map the data into a higher-dimensional space where it might become linearly separable. This allows them to learn complex non-linear decision boundaries in the original feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad25f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import f1_score, brier_score_loss\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n",
    "from scipy.stats import ks_2samp, mannwhitneyu\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "REPEATS = 5\n",
    "FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55cbb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined = pd.read_csv(\"train_processed_ver2.csv\").set_index('participant_id')   \n",
    "test_combined = pd.read_csv(\"test_processed_ver2.csv\").set_index('participant_id')  \n",
    "\n",
    "labels = pd.read_excel(\"data/TRAIN/TRAINING_SOLUTIONS.xlsx\").set_index(\"participant_id\")\n",
    "\n",
    "train_combined = train_combined.sort_index()\n",
    "labels = labels.sort_index()\n",
    "\n",
    "y_adhd = labels['ADHD_Outcome']\n",
    "y_sex = labels['Sex_F']\n",
    "\n",
    "combinations = y_adhd.astype(str) + y_sex.astype(str)\n",
    "assert all(train_combined.index == labels.index), \"Label IDs do not match train IDs\"\n",
    "\n",
    "def eval_metrics(y_true, y_pred, weights, label=\"None\", threshold=0.6):\n",
    "    brier = brier_score_loss(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, (y_pred > threshold).astype(int), sample_weight=weights)\n",
    "    print(f\"{label} -> Brier Score: {brier:.4f}, F1: {f1:.4f}\")\n",
    "    return brier, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['EHQ_EHQ_Total', 'ColorVision_CV_Score', 'APQ_P_APQ_P_CP',\n",
    "       'APQ_P_APQ_P_ID', 'APQ_P_APQ_P_INV', 'APQ_P_APQ_P_OPD',\n",
    "       'APQ_P_APQ_P_PM', 'APQ_P_APQ_P_PP', 'SDQ_SDQ_Conduct_Problems',\n",
    "       'SDQ_SDQ_Difficulties_Total', 'SDQ_SDQ_Emotional_Problems',\n",
    "       'SDQ_SDQ_Externalizing', 'SDQ_SDQ_Generating_Impact',\n",
    "       'SDQ_SDQ_Hyperactivity', 'SDQ_SDQ_Internalizing',\n",
    "       'SDQ_SDQ_Peer_Problems', 'SDQ_SDQ_Prosocial', 'MRI_Track_Age_at_Scan',\n",
    "]\n",
    "\n",
    "categorical_cols = [col for col in train_combined.columns if col not in numerical_cols]\n",
    "\n",
    "columns_to_scale = numerical_cols + ['Barratt_Barratt_P1_Edu', 'Barratt_Barratt_P1_Occ', 'Barratt_Barratt_P2_Edu', 'Barratt_Barratt_P2_Occ',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56f1beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_ord_scaler', StandardScaler(), columns_to_scale)\n",
    "    ],\n",
    "    remainder='passthrough'  # keep other columns (like one-hot) as-is\n",
    ")\n",
    "\n",
    "X_train = ct.fit_transform(train_combined)\n",
    "X_test = ct.transform(test_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1378ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the remaining column names (one-hot or untouched)\n",
    "remaining_col_names = [col for col in train_combined.columns if col not in columns_to_scale]\n",
    "\n",
    "# Full column order after 'passthrough' comes after scaled ones\n",
    "final_col_names = columns_to_scale + remaining_col_names\n",
    "\n",
    "# Rebuild DataFrames\n",
    "train_combined = pd.DataFrame(X_train, columns=final_col_names, index=train_combined.index)\n",
    "test_combined = pd.DataFrame(X_test, columns=final_col_names, index=test_combined.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eac0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_sex = ['SDQ_SDQ_Hyperactivity', 'SDQ_SDQ_Prosocial', 'SDQ_SDQ_Externalizing', 'SDQ_SDQ_Emotional_Problems', 'ColorVision_CV_Score', 'APQ_P_APQ_P_PP', 'APQ_P_APQ_P_INV', 'APQ_P_APQ_P_OPD', 'SDQ_SDQ_Internalizing']\n",
    "\n",
    "# features_sex = train_combined.columns\n",
    "\n",
    "features_sex = ['SDQ_SDQ_Emotional_Problems', 'MRI_Track_Age_at_Scan', 'APQ_P_APQ_P_INV', 'APQ_P_APQ_P_ID', 'Barratt_Barratt_P2_Occ', 'APQ_P_APQ_P_OPD', 'EHQ_EHQ_Total', 'SDQ_SDQ_Prosocial', 'SDQ_SDQ_Generating_Impact', 'APQ_P_APQ_P_PM']\n",
    "\n",
    "features_adhd = ['SDQ_SDQ_Hyperactivity', 'SDQ_SDQ_Externalizing', 'SDQ_SDQ_Difficulties_Total', 'SDQ_SDQ_Generating_Impact', 'SDQ_SDQ_Conduct_Problems', 'SDQ_SDQ_Internalizing', 'SDQ_SDQ_Peer_Problems', 'SDQ_SDQ_Emotional_Problems', 'SDQ_SDQ_Prosocial', 'APQ_P_APQ_P_OPD', 'APQ_P_APQ_P_ID', 'MRI_Track_Scan_Location_3.0', 'Basic_Demos_Enroll_Year_2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094971b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc4484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=FOLDS, n_repeats=REPEATS, random_state=SEED)\n",
    "skf = StratifiedKFold(n_splits=FOLDS)\n",
    "\n",
    "# Define weighted F1 scorer\n",
    "def weighted_f1(y_true, y_pred):\n",
    "    weights = np.where((y_true == 1) & (labels['Sex_F'].loc[y_true.index] == 1), 2, 1)\n",
    "    return f1_score(y_true, (y_pred > 0.5).astype(int), sample_weight=weights)\n",
    "\n",
    "weighted_f1_scorer = make_scorer(weighted_f1, needs_proba=True)\n",
    "\n",
    "#### CATBOOST ####\n",
    "param_grid_catboost = {\n",
    "    'iterations': [100, 500, 1000],  # select 100 200 500 1000\n",
    "    'random_state': [SEED],\n",
    "    'verbose': [0]\n",
    "}\n",
    "\n",
    "# Initialize CatBoost model for grid search\n",
    "# catboost_model = CatBoostClassifier(task_type='GPU', devices='0') # Ensure GPU usage\n",
    "\n",
    "catboost_model = CatBoostClassifier()\n",
    "    \n",
    "    \n",
    "# Initialize GridSearchCV for CatBoost\n",
    "grid_search_sex = GridSearchCV(\n",
    "    estimator=catboost_model,\n",
    "    param_grid=param_grid_catboost,\n",
    "    scoring=weighted_f1_scorer,\n",
    "    cv=rskf,\n",
    "    n_jobs=-1,  # Reduced n_jobs\n",
    ")\n",
    "\n",
    "#### LOGISTIC REGRESSION ####\n",
    "params = {\n",
    "    \"penalty\": \"l1\",\n",
    "    \"Cs\": 10,\n",
    "    \"cv\": skf,\n",
    "    \"fit_intercept\": True,\n",
    "    \"scoring\": \"f1\",\n",
    "    \"random_state\": SEED,\n",
    "    \"solver\": \"saga\",\n",
    "    \"class_weight\": \"balanced\"  \n",
    "}\n",
    "\n",
    "model_adhd = LogisticRegressionCV(**params)\n",
    "\n",
    "\n",
    "sex_oof_catboost = np.zeros(len(y_sex))\n",
    "adhd_oof = np.zeros(len(y_adhd))\n",
    "scores_sex_catboost = []\n",
    "scores_adhd = []\n",
    "\n",
    "t_adhd = 0.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba483af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the training loop for sex prediction with CatBoost\n",
    "for fold, (train_idx, val_idx) in enumerate(rskf.split(train_combined, combinations), 1):\n",
    "    print(f\"\\n=== Fold {fold} (CatBoost for Sex) ===\")\n",
    "    X_train_fold, X_val_fold = train_combined.iloc[train_idx].copy(), train_combined.iloc[val_idx].copy()\n",
    "    y_train_adhd_fold, y_val_adhd_fold = y_adhd.iloc[train_idx], y_adhd.iloc[val_idx]\n",
    "    y_train_sex_fold, y_val_sex_fold = y_sex.iloc[train_idx], y_sex.iloc[val_idx]\n",
    "\n",
    "    # Sample weights: 2x weight for \"11\" (female ADHD) cases\n",
    "    weights_train_fold = np.where(combinations.iloc[train_idx] == \"11\", 2, 1)\n",
    "    weights_val_fold = np.where(combinations.iloc[val_idx] == \"11\", 2, 1)\n",
    "\n",
    "    # ----- First Stage: ADHD Model (Keep as is) -----\n",
    "    model_adhd.fit(X_train_fold[features_adhd], y_train_adhd_fold, sample_weight=weights_train_fold)\n",
    "    adhd_val_fold = model_adhd.predict_proba(X_val_fold[features_adhd])[:, 1]\n",
    "    adhd_brier, adhd_f1 = eval_metrics(y_val_adhd_fold, adhd_val_fold, weights_val_fold, \"ADHD\", t_adhd)\n",
    "\n",
    "    # ----- Second Stage: Sex Model (CatBoost with Grid Search) -----\n",
    "    grid_search_sex.fit(X_train_fold[features_sex], y_train_sex_fold, sample_weight=weights_train_fold)\n",
    "    best_catboost_model = grid_search_sex.best_estimator_\n",
    "    sex_val_catboost = best_catboost_model.predict_proba(X_val_fold[features_sex])[:, 1]\n",
    "    sex_oof_catboost[val_idx] += sex_val_catboost / REPEATS\n",
    "\n",
    "    # Evaluate CatBoost model\n",
    "    sex_brier_catboost, sex_f1_catboost = eval_metrics(y_val_sex_fold, sex_val_catboost, weights_val_fold, \"Sex_F (CatBoost)\")\n",
    "    scores_sex_catboost.append((sex_brier_catboost, sex_f1_catboost))\n",
    "\n",
    "print(f\"\\n=== CV Results (CatBoost for Sex) ===\")\n",
    "print(f\"Sex Mean Brier Score (CatBoost): {np.mean([s[0] for s in scores_sex_catboost]):.4f}\")\n",
    "print(f\"Sex Mean F1 (CatBoost): {np.mean([s[1] for s in scores_sex_catboost]):.4f}\")\n",
    "print(f\"ADHD Mean Brier Score (Logistic Regression): {np.mean([s[0] for s in scores_adhd]):.4f}\")\n",
    "print(f\"ADHD Mean F1 (Logistic Regression): {np.mean([s[1] for s in scores_adhd]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7fbd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_catboost_model.get_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d12073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final training of both models on the entire training data\n",
    "weights_full = np.where(combinations == \"11\", 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa1c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final Logistic Regression model for ADHD\n",
    "final_model_adhd = LogisticRegressionCV(**params)\n",
    "final_model_adhd.fit(train_combined[features_adhd], y_adhd, sample_weight=weights_full)\n",
    "adhd_proba_test_final = final_model_adhd.predict_proba(test_combined[features_adhd])[:, 1]\n",
    "\n",
    "# Train final CatBoost model for Sex using the best parameters found by grid search\n",
    "final_model_sex_catboost = grid_search_sex.best_estimator_\n",
    "final_model_sex_catboost.fit(train_combined[features_sex], y_sex, sample_weight=weights_full)\n",
    "sex_proba_test_final_catboost = final_model_sex_catboost.predict_proba(test_combined[features_sex])[:, 1]\n",
    "\n",
    "# Threshold optimization for OOF predictions (using CatBoost OOF for sex)\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "\n",
    "# ADHD threshold optimization (using existing OOF predictions)\n",
    "adhd_scores = [f1_score(y_adhd, (adhd_oof > t).astype(int), sample_weight=weights_full) for t in thresholds]\n",
    "best_adhd_threshold = thresholds[np.argmax(adhd_scores)]\n",
    "\n",
    "# Sex threshold optimization (using CatBoost OOF predictions)\n",
    "sex_scores_catboost_oof = [f1_score(y_sex, (sex_oof_catboost > t).astype(int), sample_weight=weights_full) for t in thresholds]\n",
    "best_sex_threshold_catboost = thresholds[np.argmax(sex_scores_catboost_oof)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9ad97b",
   "metadata": {},
   "source": [
    "# 5. Inference / Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8005fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission_catboost = pd.read_excel(\"data/SAMPLE_SUBMISSION.xlsx\")\n",
    "submission_catboost[\"ADHD_Outcome\"] = np.where(adhd_proba_test_final > best_adhd_threshold, 1, 0)\n",
    "submission_catboost[\"Sex_F\"] = np.where(sex_proba_test_final_catboost > best_sex_threshold_catboost, 1, 0)\n",
    "\n",
    "# Compare share of predicted labels at thresholds between OOF and Test (for CatBoost Sex)\n",
    "print(f\"Share ADHD OOF: {np.mean(np.where(adhd_oof > best_adhd_threshold, 1, 0)):.4f} - Share ADHD Test: {submission_catboost.ADHD_Outcome.mean():.4f}\")\n",
    "print(f\"Share Sex_F OOF (CatBoost): {np.mean(np.where(sex_oof_catboost > best_sex_threshold_catboost, 1, 0)):.4f} - Share Sex_F Test (CatBoost): {submission_catboost.Sex_F.mean():.4f}\")\n",
    "\n",
    "submission_catboost.to_csv(\"submission_catboost.csv\", index=False)\n",
    "\n",
    "print(\"\\nCatBoost grid search complete and submission file generated.\")\n",
    "print(\"Best parameters found for CatBoost (Sex):\", grid_search_sex.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33116fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
